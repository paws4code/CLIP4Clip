{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "4b8b62b18ed2428ca96b1e08f531b1da": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_f43047f8502b4a70a6e2ecae958b4203",
              "IPY_MODEL_c1d8db25c84744d4a1b8df2b9ee57a8a",
              "IPY_MODEL_98246ae331b04e61b9db992bc588c1a4"
            ],
            "layout": "IPY_MODEL_9ce6e5af21dc498fbb79127f6d509a7b"
          }
        },
        "f43047f8502b4a70a6e2ecae958b4203": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_c160f1d705a144ce934dd4d706a4f36c",
            "placeholder": "​",
            "style": "IPY_MODEL_e35d9d84758b49d8b8b21eaeeaaa3a80",
            "value": "Epoch 1/2 [Train]:   0%"
          }
        },
        "c1d8db25c84744d4a1b8df2b9ee57a8a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "danger",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_9e45f5f3e63b49e6ab6e6b09595852b9",
            "max": 2,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_4edb7afef49242fdba055647c1d63efb",
            "value": 0
          }
        },
        "98246ae331b04e61b9db992bc588c1a4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_d274f5da45c048e8b8e84d9320b1b0df",
            "placeholder": "​",
            "style": "IPY_MODEL_c05d9d3a9c094c4395f1931d53056024",
            "value": " 0/2 [00:00&lt;?, ?it/s]"
          }
        },
        "9ce6e5af21dc498fbb79127f6d509a7b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c160f1d705a144ce934dd4d706a4f36c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e35d9d84758b49d8b8b21eaeeaaa3a80": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "9e45f5f3e63b49e6ab6e6b09595852b9": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "4edb7afef49242fdba055647c1d63efb": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "d274f5da45c048e8b8e84d9320b1b0df": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c05d9d3a9c094c4395f1931d53056024": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/paws4code/CLIP4Clip/blob/master/CLIP4Clip.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# First remove everything\n",
        "!rm -rf CLIP4Clip*\n",
        "\n",
        "# Clone fresh\n",
        "!git clone https://github.com/ArrowLuo/CLIP4Clip.git\n",
        "\n",
        "# Change directory into the single CLIP4Clip folder\n",
        "%cd CLIP4Clip\n",
        "\n",
        "# Verify current directory structure (should show only one CLIP4Clip level)\n",
        "!pwd\n",
        "!ls\n",
        "\n",
        "# Now continue with setup as before\n",
        "!mkdir -p modules MSRVTT_Videos\n",
        "!wget https://github.com/ArrowLuo/CLIP4Clip/releases/download/v0.0/msrvtt_data.zip\n",
        "!unzip -o msrvtt_data.zip\n",
        "\n",
        "# Move files to correct location\n",
        "!mv msrvtt_data/MSRVTT_*.csv ./\n",
        "!mv msrvtt_data/MSRVTT_data.json ./\n",
        "\n",
        "# Clean up zip and extracted folder\n",
        "!rm msrvtt_data.zip\n",
        "!rm -rf msrvtt_data\n",
        "\n",
        "# Download CLIP model\n",
        "!wget -P ./modules https://openaipublic.azureedge.net/clip/models/40d365715913c9da98579312b702a82c18be219cc2a73407c4526f58eba950af/ViT-B-32.pt"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bX9uPiwwBoTS",
        "outputId": "1b7c6958-b246-4cb4-dc52-5414d0b66b42"
      },
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'CLIP4Clip'...\n",
            "remote: Enumerating objects: 119, done.\u001b[K\n",
            "remote: Counting objects: 100% (58/58), done.\u001b[K\n",
            "remote: Compressing objects: 100% (29/29), done.\u001b[K\n",
            "remote: Total 119 (delta 43), reused 29 (delta 29), pack-reused 61 (from 1)\u001b[K\n",
            "Receiving objects: 100% (119/119), 1.67 MiB | 4.52 MiB/s, done.\n",
            "Resolving deltas: 100% (57/57), done.\n",
            "/content/CLIP4Clip\n",
            "/content/CLIP4Clip\n",
            "CLIP4Clip.png  LICENSE\t\t       metrics.py  preprocess  util.py\n",
            "dataloaders    main_task_retrieval.py  modules\t   README.md\n",
            "--2025-03-09 21:27:53--  https://github.com/ArrowLuo/CLIP4Clip/releases/download/v0.0/msrvtt_data.zip\n",
            "Resolving github.com (github.com)... 140.82.112.4\n",
            "Connecting to github.com (github.com)|140.82.112.4|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://objects.githubusercontent.com/github-production-release-asset-2e65be/357478494/05e8fe00-a387-11eb-8f43-625d90bce31f?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=releaseassetproduction%2F20250309%2Fus-east-1%2Fs3%2Faws4_request&X-Amz-Date=20250309T212738Z&X-Amz-Expires=300&X-Amz-Signature=eec1e0377ba30a6eeee086f545e92ffef711b82288858f26f9812e0c14f88eb9&X-Amz-SignedHeaders=host&response-content-disposition=attachment%3B%20filename%3Dmsrvtt_data.zip&response-content-type=application%2Foctet-stream [following]\n",
            "--2025-03-09 21:27:53--  https://objects.githubusercontent.com/github-production-release-asset-2e65be/357478494/05e8fe00-a387-11eb-8f43-625d90bce31f?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=releaseassetproduction%2F20250309%2Fus-east-1%2Fs3%2Faws4_request&X-Amz-Date=20250309T212738Z&X-Amz-Expires=300&X-Amz-Signature=eec1e0377ba30a6eeee086f545e92ffef711b82288858f26f9812e0c14f88eb9&X-Amz-SignedHeaders=host&response-content-disposition=attachment%3B%20filename%3Dmsrvtt_data.zip&response-content-type=application%2Foctet-stream\n",
            "Resolving objects.githubusercontent.com (objects.githubusercontent.com)... 185.199.111.133, 185.199.110.133, 185.199.108.133, ...\n",
            "Connecting to objects.githubusercontent.com (objects.githubusercontent.com)|185.199.111.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 4067617 (3.9M) [application/octet-stream]\n",
            "Saving to: ‘msrvtt_data.zip’\n",
            "\n",
            "msrvtt_data.zip     100%[===================>]   3.88M  --.-KB/s    in 0.05s   \n",
            "\n",
            "2025-03-09 21:27:54 (81.5 MB/s) - ‘msrvtt_data.zip’ saved [4067617/4067617]\n",
            "\n",
            "Archive:  msrvtt_data.zip\n",
            "   creating: msrvtt_data/\n",
            "  inflating: msrvtt_data/MSRVTT_data.json  \n",
            "  inflating: msrvtt_data/MSRVTT_JSFUSION_test.csv  \n",
            "  inflating: msrvtt_data/MSRVTT_train.7k.csv  \n",
            "  inflating: msrvtt_data/MSRVTT_train.9k.csv  \n",
            "--2025-03-09 21:27:54--  https://openaipublic.azureedge.net/clip/models/40d365715913c9da98579312b702a82c18be219cc2a73407c4526f58eba950af/ViT-B-32.pt\n",
            "Resolving openaipublic.azureedge.net (openaipublic.azureedge.net)... 13.107.246.70, 2620:1ec:bdf::70\n",
            "Connecting to openaipublic.azureedge.net (openaipublic.azureedge.net)|13.107.246.70|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 353976522 (338M) [application/octet-stream]\n",
            "Saving to: ‘./modules/ViT-B-32.pt’\n",
            "\n",
            "ViT-B-32.pt         100%[===================>] 337.58M  35.6MB/s    in 8.0s    \n",
            "\n",
            "2025-03-09 21:28:02 (42.2 MB/s) - ‘./modules/ViT-B-32.pt’ saved [353976522/353976522]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# First, go back to the root directory\n",
        "%cd /content\n",
        "\n",
        "# Remove all nested CLIP4Clip directories\n",
        "!rm -rf CLIP4Clip*\n",
        "\n",
        "# Clone fresh\n",
        "!git clone https://github.com/ArrowLuo/CLIP4Clip.git\n",
        "\n",
        "# Change directory into the single CLIP4Clip folder and verify location\n",
        "%cd CLIP4Clip\n",
        "!pwd\n",
        "\n",
        "# Now continue with setup\n",
        "!mkdir -p modules MSRVTT_Videos\n",
        "!wget https://github.com/ArrowLuo/CLIP4Clip/releases/download/v0.0/msrvtt_data.zip\n",
        "!unzip -o msrvtt_data.zip"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f6smSSONFG18",
        "outputId": "df12b086-d3d1-4627-9e55-13851516af5c"
      },
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content\n",
            "Cloning into 'CLIP4Clip'...\n",
            "remote: Enumerating objects: 119, done.\u001b[K\n",
            "remote: Counting objects: 100% (58/58), done.\u001b[K\n",
            "remote: Compressing objects: 100% (29/29), done.\u001b[K\n",
            "remote: Total 119 (delta 43), reused 29 (delta 29), pack-reused 61 (from 1)\u001b[K\n",
            "Receiving objects: 100% (119/119), 1.67 MiB | 4.46 MiB/s, done.\n",
            "Resolving deltas: 100% (57/57), done.\n",
            "/content/CLIP4Clip\n",
            "/content/CLIP4Clip\n",
            "--2025-03-09 21:28:06--  https://github.com/ArrowLuo/CLIP4Clip/releases/download/v0.0/msrvtt_data.zip\n",
            "Resolving github.com (github.com)... 140.82.113.4\n",
            "Connecting to github.com (github.com)|140.82.113.4|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://objects.githubusercontent.com/github-production-release-asset-2e65be/357478494/05e8fe00-a387-11eb-8f43-625d90bce31f?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=releaseassetproduction%2F20250309%2Fus-east-1%2Fs3%2Faws4_request&X-Amz-Date=20250309T212807Z&X-Amz-Expires=300&X-Amz-Signature=167353b05667e30c101f3b1aad49850b52df4864dbf319de704f47696378a7e2&X-Amz-SignedHeaders=host&response-content-disposition=attachment%3B%20filename%3Dmsrvtt_data.zip&response-content-type=application%2Foctet-stream [following]\n",
            "--2025-03-09 21:28:07--  https://objects.githubusercontent.com/github-production-release-asset-2e65be/357478494/05e8fe00-a387-11eb-8f43-625d90bce31f?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=releaseassetproduction%2F20250309%2Fus-east-1%2Fs3%2Faws4_request&X-Amz-Date=20250309T212807Z&X-Amz-Expires=300&X-Amz-Signature=167353b05667e30c101f3b1aad49850b52df4864dbf319de704f47696378a7e2&X-Amz-SignedHeaders=host&response-content-disposition=attachment%3B%20filename%3Dmsrvtt_data.zip&response-content-type=application%2Foctet-stream\n",
            "Resolving objects.githubusercontent.com (objects.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to objects.githubusercontent.com (objects.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 4067617 (3.9M) [application/octet-stream]\n",
            "Saving to: ‘msrvtt_data.zip’\n",
            "\n",
            "msrvtt_data.zip     100%[===================>]   3.88M  --.-KB/s    in 0.05s   \n",
            "\n",
            "2025-03-09 21:28:07 (71.3 MB/s) - ‘msrvtt_data.zip’ saved [4067617/4067617]\n",
            "\n",
            "Archive:  msrvtt_data.zip\n",
            "   creating: msrvtt_data/\n",
            "  inflating: msrvtt_data/MSRVTT_data.json  \n",
            "  inflating: msrvtt_data/MSRVTT_JSFUSION_test.csv  \n",
            "  inflating: msrvtt_data/MSRVTT_train.7k.csv  \n",
            "  inflating: msrvtt_data/MSRVTT_train.9k.csv  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pwd"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 56
        },
        "id": "BmCvTDQiFPnq",
        "outputId": "cf39f450-b80f-488a-c4fa-eba8d31cf14d"
      },
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'/content/CLIP4Clip'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 39
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "ls"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OsfxzrczFQJ3",
        "outputId": "df983042-c19a-415c-cb28-0ba469eee97b"
      },
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CLIP4Clip.png  LICENSE                 metrics.py  \u001b[0m\u001b[01;34mmsrvtt_data\u001b[0m/     \u001b[01;34mMSRVTT_Videos\u001b[0m/  README.md\n",
            "\u001b[01;34mdataloaders\u001b[0m/   main_task_retrieval.py  \u001b[01;34mmodules\u001b[0m/    msrvtt_data.zip  \u001b[01;34mpreprocess\u001b[0m/     util.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# First, download MSRVTT data again\n",
        "!wget https://github.com/ArrowLuo/CLIP4Clip/releases/download/v0.0/msrvtt_data.zip\n",
        "!unzip MSRVTT.zip -d MSRVTT_Videos/\n",
        "\n",
        "# Then run the rest of the setup\n",
        "!unzip -o msrvtt_data.zip\n",
        "!mv msrvtt_data/MSRVTT_*.csv ./\n",
        "!mv msrvtt_data/MSRVTT_data.json ./\n",
        "!rm -rf msrvtt_data.zip msrvtt_data\n",
        "\n",
        "# Run evaluation with fixed parameters\n",
        "!python main_task_retrieval.py \\\n",
        "    --do_eval \\\n",
        "    --num_thread_reader=0 \\\n",
        "    --batch_size=128 \\\n",
        "    --batch_size_val=16 \\\n",
        "    --datatype msrvtt \\\n",
        "    --features_path MSRVTT_Videos/MSRVTT/videos/all \\\n",
        "    --data_path MSRVTT_data.json \\\n",
        "    --val_csv MSRVTT_JSFUSION_test.csv \\\n",
        "    --train_csv MSRVTT_train.9k.csv \\\n",
        "    --pretrained_clip_name ViT-B/32 \\\n",
        "    --output_dir ckpts/ckpt_msrvtt_retrieval_looseType \\\n",
        "    --loose_type \\\n",
        "    --linear_patch 2d \\\n",
        "    --sim_header meanP"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CJ6Fg2hAFRQJ",
        "outputId": "5ddb513f-a9b1-4680-b6fa-efaee0cf438e"
      },
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2025-03-09 21:29:52--  https://github.com/ArrowLuo/CLIP4Clip/releases/download/v0.0/msrvtt_data.zip\n",
            "Resolving github.com (github.com)... 140.82.112.4\n",
            "Connecting to github.com (github.com)|140.82.112.4|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://objects.githubusercontent.com/github-production-release-asset-2e65be/357478494/05e8fe00-a387-11eb-8f43-625d90bce31f?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=releaseassetproduction%2F20250309%2Fus-east-1%2Fs3%2Faws4_request&X-Amz-Date=20250309T212952Z&X-Amz-Expires=300&X-Amz-Signature=719f5e66afef54942b321b366f866b8f3c32b49ece1328ec79760bbecbf23cd2&X-Amz-SignedHeaders=host&response-content-disposition=attachment%3B%20filename%3Dmsrvtt_data.zip&response-content-type=application%2Foctet-stream [following]\n",
            "--2025-03-09 21:29:52--  https://objects.githubusercontent.com/github-production-release-asset-2e65be/357478494/05e8fe00-a387-11eb-8f43-625d90bce31f?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=releaseassetproduction%2F20250309%2Fus-east-1%2Fs3%2Faws4_request&X-Amz-Date=20250309T212952Z&X-Amz-Expires=300&X-Amz-Signature=719f5e66afef54942b321b366f866b8f3c32b49ece1328ec79760bbecbf23cd2&X-Amz-SignedHeaders=host&response-content-disposition=attachment%3B%20filename%3Dmsrvtt_data.zip&response-content-type=application%2Foctet-stream\n",
            "Resolving objects.githubusercontent.com (objects.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to objects.githubusercontent.com (objects.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 4067617 (3.9M) [application/octet-stream]\n",
            "Saving to: ‘msrvtt_data.zip’\n",
            "\n",
            "msrvtt_data.zip     100%[===================>]   3.88M  --.-KB/s    in 0.06s   \n",
            "\n",
            "2025-03-09 21:29:53 (67.6 MB/s) - ‘msrvtt_data.zip’ saved [4067617/4067617]\n",
            "\n",
            "unzip:  cannot find or open MSRVTT.zip, MSRVTT.zip.zip or MSRVTT.zip.ZIP.\n",
            "Archive:  msrvtt_data.zip\n",
            "   creating: msrvtt_data/\n",
            "  inflating: msrvtt_data/MSRVTT_data.json  \n",
            "  inflating: msrvtt_data/MSRVTT_JSFUSION_test.csv  \n",
            "  inflating: msrvtt_data/MSRVTT_train.7k.csv  \n",
            "  inflating: msrvtt_data/MSRVTT_train.9k.csv  \n",
            "03/09/2025 21:29:57 - INFO -   Effective parameters:\n",
            "03/09/2025 21:29:57 - INFO -     <<< batch_size: 128\n",
            "03/09/2025 21:29:57 - INFO -     <<< batch_size_val: 16\n",
            "03/09/2025 21:29:57 - INFO -     <<< cache_dir: \n",
            "03/09/2025 21:29:57 - INFO -     <<< coef_lr: 1.0\n",
            "03/09/2025 21:29:57 - INFO -     <<< cross_model: cross-base\n",
            "03/09/2025 21:29:57 - INFO -     <<< cross_num_hidden_layers: 4\n",
            "03/09/2025 21:29:57 - INFO -     <<< data_path: MSRVTT_data.json\n",
            "03/09/2025 21:29:57 - INFO -     <<< datatype: msrvtt\n",
            "03/09/2025 21:29:57 - INFO -     <<< do_eval: True\n",
            "03/09/2025 21:29:57 - INFO -     <<< do_lower_case: False\n",
            "03/09/2025 21:29:57 - INFO -     <<< do_pretrain: False\n",
            "03/09/2025 21:29:57 - INFO -     <<< do_train: False\n",
            "03/09/2025 21:29:57 - INFO -     <<< epochs: 20\n",
            "03/09/2025 21:29:57 - INFO -     <<< eval_frame_order: 0\n",
            "03/09/2025 21:29:57 - INFO -     <<< expand_msrvtt_sentences: False\n",
            "03/09/2025 21:29:57 - INFO -     <<< feature_framerate: 1\n",
            "03/09/2025 21:29:57 - INFO -     <<< features_path: MSRVTT_Videos/MSRVTT/videos/all\n",
            "03/09/2025 21:29:57 - INFO -     <<< fp16: False\n",
            "03/09/2025 21:29:57 - INFO -     <<< fp16_opt_level: O1\n",
            "03/09/2025 21:29:57 - INFO -     <<< freeze_layer_num: 0\n",
            "03/09/2025 21:29:57 - INFO -     <<< gradient_accumulation_steps: 1\n",
            "03/09/2025 21:29:57 - INFO -     <<< hard_negative_rate: 0.5\n",
            "03/09/2025 21:29:57 - INFO -     <<< init_model: None\n",
            "03/09/2025 21:29:57 - INFO -     <<< linear_patch: 2d\n",
            "03/09/2025 21:29:57 - INFO -     <<< local_rank: 0\n",
            "03/09/2025 21:29:57 - INFO -     <<< loose_type: True\n",
            "03/09/2025 21:29:57 - INFO -     <<< lr: 0.0001\n",
            "03/09/2025 21:29:57 - INFO -     <<< lr_decay: 0.9\n",
            "03/09/2025 21:29:57 - INFO -     <<< margin: 0.1\n",
            "03/09/2025 21:29:57 - INFO -     <<< max_frames: 100\n",
            "03/09/2025 21:29:57 - INFO -     <<< max_words: 20\n",
            "03/09/2025 21:29:57 - INFO -     <<< n_display: 100\n",
            "03/09/2025 21:29:57 - INFO -     <<< n_gpu: 1\n",
            "03/09/2025 21:29:57 - INFO -     <<< n_pair: 1\n",
            "03/09/2025 21:29:57 - INFO -     <<< negative_weighting: 1\n",
            "03/09/2025 21:29:57 - INFO -     <<< num_thread_reader: 0\n",
            "03/09/2025 21:29:57 - INFO -     <<< output_dir: ckpts/ckpt_msrvtt_retrieval_looseType\n",
            "03/09/2025 21:29:57 - INFO -     <<< pretrained_clip_name: ViT-B/32\n",
            "03/09/2025 21:29:57 - INFO -     <<< rank: 0\n",
            "03/09/2025 21:29:57 - INFO -     <<< resume_model: None\n",
            "03/09/2025 21:29:57 - INFO -     <<< sampled_use_mil: False\n",
            "03/09/2025 21:29:57 - INFO -     <<< seed: 42\n",
            "03/09/2025 21:29:57 - INFO -     <<< sim_header: meanP\n",
            "03/09/2025 21:29:57 - INFO -     <<< slice_framepos: 0\n",
            "03/09/2025 21:29:57 - INFO -     <<< task_type: retrieval\n",
            "03/09/2025 21:29:57 - INFO -     <<< text_num_hidden_layers: 12\n",
            "03/09/2025 21:29:57 - INFO -     <<< train_csv: MSRVTT_train.9k.csv\n",
            "03/09/2025 21:29:57 - INFO -     <<< train_frame_order: 0\n",
            "03/09/2025 21:29:57 - INFO -     <<< use_mil: False\n",
            "03/09/2025 21:29:57 - INFO -     <<< val_csv: MSRVTT_JSFUSION_test.csv\n",
            "03/09/2025 21:29:57 - INFO -     <<< video_dim: 1024\n",
            "03/09/2025 21:29:57 - INFO -     <<< visual_num_hidden_layers: 12\n",
            "03/09/2025 21:29:57 - INFO -     <<< warmup_proportion: 0.1\n",
            "03/09/2025 21:29:57 - INFO -     <<< world_size: 1\n",
            "03/09/2025 21:29:57 - INFO -   device: cuda:0 n_gpu: 1\n",
            "03/09/2025 21:29:59 - INFO -   loading archive file /content/CLIP4Clip/modules/cross-base\n",
            "03/09/2025 21:29:59 - INFO -   Model config {\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 512,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 2048,\n",
            "  \"max_position_embeddings\": 128,\n",
            "  \"num_attention_heads\": 8,\n",
            "  \"num_hidden_layers\": 4,\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"vocab_size\": 512\n",
            "}\n",
            "\n",
            "03/09/2025 21:29:59 - INFO -   Weight doesn't exsits. /content/CLIP4Clip/modules/cross-base/cross_pytorch_model.bin\n",
            "03/09/2025 21:29:59 - WARNING -   Stage-One:True, Stage-Two:False\n",
            "03/09/2025 21:29:59 - WARNING -   Test retrieval by loose type.\n",
            "03/09/2025 21:29:59 - WARNING -   \t embed_dim: 512\n",
            "03/09/2025 21:29:59 - WARNING -   \t image_resolution: 224\n",
            "03/09/2025 21:29:59 - WARNING -   \t vision_layers: 12\n",
            "03/09/2025 21:29:59 - WARNING -   \t vision_width: 768\n",
            "03/09/2025 21:29:59 - WARNING -   \t vision_patch_size: 32\n",
            "03/09/2025 21:29:59 - WARNING -   \t context_length: 77\n",
            "03/09/2025 21:29:59 - WARNING -   \t vocab_size: 49408\n",
            "03/09/2025 21:29:59 - WARNING -   \t transformer_width: 512\n",
            "03/09/2025 21:29:59 - WARNING -   \t transformer_heads: 8\n",
            "03/09/2025 21:29:59 - WARNING -   \t transformer_layers: 12\n",
            "03/09/2025 21:29:59 - WARNING -   \t\t linear_patch: 2d\n",
            "03/09/2025 21:29:59 - WARNING -   \t cut_top_layer: 0\n",
            "03/09/2025 21:30:01 - WARNING -   \t sim_header: meanP\n",
            "03/09/2025 21:30:05 - INFO -   --------------------\n",
            "03/09/2025 21:30:05 - INFO -   Weights from pretrained model not used in CLIP4Clip: \n",
            "   clip.input_resolution\n",
            "   clip.context_length\n",
            "   clip.vocab_size\n",
            "03/09/2025 21:30:05 - INFO -   ***** Running test *****\n",
            "03/09/2025 21:30:05 - INFO -     Num examples = 1000\n",
            "03/09/2025 21:30:05 - INFO -     Batch size = 16\n",
            "03/09/2025 21:30:05 - INFO -     Num steps = 63\n",
            "03/09/2025 21:30:05 - INFO -   ***** Running val *****\n",
            "03/09/2025 21:30:05 - INFO -     Num examples = 1000\n",
            "[rank0]: Traceback (most recent call last):\n",
            "[rank0]:   File \"/content/CLIP4Clip/main_task_retrieval.py\", line 582, in <module>\n",
            "[rank0]:     main()\n",
            "[rank0]:   File \"/content/CLIP4Clip/main_task_retrieval.py\", line 579, in main\n",
            "[rank0]:     eval_epoch(args, model, test_dataloader, device, n_gpu)\n",
            "[rank0]:   File \"/content/CLIP4Clip/main_task_retrieval.py\", line 359, in eval_epoch\n",
            "[rank0]:     for bid, batch in enumerate(test_dataloader):\n",
            "[rank0]:   File \"/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataloader.py\", line 701, in __next__\n",
            "[rank0]:     data = self._next_data()\n",
            "[rank0]:            ^^^^^^^^^^^^^^^^^\n",
            "[rank0]:   File \"/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataloader.py\", line 757, in _next_data\n",
            "[rank0]:     data = self._dataset_fetcher.fetch(index)  # may raise StopIteration\n",
            "[rank0]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "[rank0]:   File \"/usr/local/lib/python3.11/dist-packages/torch/utils/data/_utils/fetch.py\", line 52, in fetch\n",
            "[rank0]:     data = [self.dataset[idx] for idx in possibly_batched_index]\n",
            "[rank0]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "[rank0]:   File \"/usr/local/lib/python3.11/dist-packages/torch/utils/data/_utils/fetch.py\", line 52, in <listcomp>\n",
            "[rank0]:     data = [self.dataset[idx] for idx in possibly_batched_index]\n",
            "[rank0]:             ~~~~~~~~~~~~^^^^^\n",
            "[rank0]:   File \"/content/CLIP4Clip/dataloaders/dataloader_msrvtt_retrieval.py\", line 136, in __getitem__\n",
            "[rank0]:     video, video_mask = self._get_rawvideo(choice_video_ids)\n",
            "[rank0]:                         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "[rank0]:   File \"/content/CLIP4Clip/dataloaders/dataloader_msrvtt_retrieval.py\", line 98, in _get_rawvideo\n",
            "[rank0]:     raw_video_data = self.rawVideoExtractor.get_video_data(video_path)\n",
            "[rank0]:                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "[rank0]:   File \"/content/CLIP4Clip/dataloaders/rawvideo_util.py\", line 76, in get_video_data\n",
            "[rank0]:     image_input = self.video_to_tensor(video_path, self.transform, sample_fp=self.framerate, start_time=start_time, end_time=end_time)\n",
            "[rank0]:                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "[rank0]:   File \"/content/CLIP4Clip/dataloaders/rawvideo_util.py\", line 36, in video_to_tensor\n",
            "[rank0]:     total_duration = (frameCount + fps - 1) // fps\n",
            "[rank0]:                      ~~~~~~~~~~~~~~~~~~~~~~~^^~~~~\n",
            "[rank0]: ZeroDivisionError: integer division or modulo by zero\n",
            "[rank0]:[W309 21:30:05.964926382 ProcessGroupNCCL.cpp:1250] Warning: WARNING: process group has NOT been destroyed before we destruct ProcessGroupNCCL. On normal program exit, the application should call destroy_process_group to ensure that any pending NCCL operations have finished in this process. In rare cases this process can exit before this point and block the progress of another member of the process group. This constraint has always been present,  but this warning has only been added since PyTorch 2.4 (function operator())\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python main_task_retrieval.py \\\n",
        "    --do_eval \\\n",
        "    --num_thread_reader=1 \\\n",
        "    --batch_size=128 \\\n",
        "    --batch_size_val=16 \\\n",
        "    --datatype msrvtt \\\n",
        "    --features_path \"MSRVTT_Videos/MSRVTT/videos/all\" \\\n",
        "    --data_path \"MSRVTT_data.json\" \\\n",
        "    --val_csv \"MSRVTT_JSFUSION_test.csv\" \\\n",
        "    --train_csv \"MSRVTT_train.9k.csv\" \\\n",
        "    --pretrained_clip_name \"ViT-B/32\" \\\n",
        "    --output_dir \"ckpts/ckpt_msrvtt_retrieval_looseType\" \\\n",
        "    --loose_type \\\n",
        "    --linear_patch 2d \\\n",
        "    --sim_header meanP\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4hO78i4JoReO",
        "outputId": "b26b2306-ac6e-4857-ba93-670378276473"
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "python3: can't open file '/content/main_task_retrieval.py': [Errno 2] No such file or directory\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "ls"
      ],
      "metadata": {
        "id": "GrfAFqXwxni1",
        "outputId": "16c5f2ca-7f46-4ec2-b365-1134f052732c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[0m\u001b[01;34mCLIP4Clip\u001b[0m/        MSRVTT_JSFUSION_test.csv  MSRVTT_train.9k.csv\n",
            "MSRVTT_data.json  MSRVTT_train.7k.csv       \u001b[01;34msample_data\u001b[0m/\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# %% [markdown]\n",
        "# # Video-to-Text Generation with Panda-70M\n",
        "#\n",
        "# This notebook demonstrates how to train a video-to-text generation model using the Panda-70M dataset. We'll create a model that can generate descriptive captions for videos.\n",
        "\n",
        "# %% [markdown]\n",
        "# ## 1. Setup and Dependencies\n",
        "\n",
        "# %%\n",
        "# Install required packages\n",
        "!pip install torch torchvision transformers tqdm matplotlib pandas opencv-python\n",
        "\n",
        "# %%\n",
        "import os\n",
        "import torch\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM, AutoProcessor, AutoModelForVision2Seq\n",
        "from transformers import get_linear_schedule_with_warmup\n",
        "import torchvision.transforms as transforms\n",
        "from torch.optim import AdamW\n",
        "from torch.optim.lr_scheduler import CosineAnnealingLR\n",
        "from tqdm.auto import tqdm\n",
        "import json\n",
        "import cv2\n",
        "from PIL import Image\n",
        "import matplotlib.pyplot as plt\n",
        "import gc\n",
        "import logging\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "# Set up logging\n",
        "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
        "logger = logging.getLogger(__name__)\n",
        "\n",
        "# Check if CUDA is available\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\"Using device: {device}\")\n",
        "\n",
        "# %% [markdown]\n",
        "# ## 2. Dataset Implementation\n",
        "#\n",
        "# First, let's implement a dataset class for Panda-70M that handles video loading and preprocessing.\n",
        "\n",
        "# %%\n",
        "class Panda70MDataset(Dataset):\n",
        "    def __init__(self, root_dir, processor, max_frames=8, frame_sampling='uniform',\n",
        "                 max_text_length=128, split='train', sample_size=None):\n",
        "        \"\"\"\n",
        "        Dataset for Panda-70M video-to-text generation\n",
        "\n",
        "        Args:\n",
        "            root_dir: Root directory of Panda-70M dataset\n",
        "            processor: Processor for video and text processing\n",
        "            max_frames: Maximum number of frames to sample from each video\n",
        "            frame_sampling: Strategy to sample frames ('uniform', 'first', 'random')\n",
        "            max_text_length: Maximum text length for tokenization\n",
        "            split: 'train', 'val', or 'test'\n",
        "            sample_size: Number of samples to use (for testing, None for all)\n",
        "        \"\"\"\n",
        "        self.root_dir = root_dir\n",
        "        self.processor = processor\n",
        "        self.max_frames = max_frames\n",
        "        self.frame_sampling = frame_sampling\n",
        "        self.max_text_length = max_text_length\n",
        "        self.split = split\n",
        "\n",
        "        # Load metadata\n",
        "        metadata_file = os.path.join(root_dir, f\"{split}_metadata.json\")\n",
        "        if not os.path.exists(metadata_file):\n",
        "            metadata_file = os.path.join(root_dir, \"metadata.json\")  # Fallback to single metadata file\n",
        "\n",
        "        if not os.path.exists(metadata_file):\n",
        "            # Further fallback: try to find any metadata file\n",
        "            potential_files = [\n",
        "                os.path.join(root_dir, \"metadata.jsonl\"),\n",
        "                os.path.join(root_dir, \"captions.json\"),\n",
        "                os.path.join(root_dir, \"annotations.json\")\n",
        "            ]\n",
        "            for file in potential_files:\n",
        "                if os.path.exists(file):\n",
        "                    metadata_file = file\n",
        "                    break\n",
        "\n",
        "        logger.info(f\"Loading metadata from {metadata_file}\")\n",
        "\n",
        "        # Load based on file extension\n",
        "        if metadata_file.endswith('.jsonl'):\n",
        "            self.metadata = []\n",
        "            with open(metadata_file, 'r') as f:\n",
        "                for line in f:\n",
        "                    self.metadata.append(json.loads(line))\n",
        "        else:\n",
        "            with open(metadata_file, 'r') as f:\n",
        "                self.metadata = json.load(f)\n",
        "\n",
        "        # Handle different metadata formats\n",
        "        if isinstance(self.metadata, dict):\n",
        "            # Convert dict to list if needed\n",
        "            if 'videos' in self.metadata:\n",
        "                self.metadata = self.metadata['videos']\n",
        "            elif 'annotations' in self.metadata:\n",
        "                self.metadata = self.metadata['annotations']\n",
        "            else:\n",
        "                # Try to convert dict to list\n",
        "                self.metadata = [{'id': k, **v} for k, v in self.metadata.items()]\n",
        "\n",
        "        logger.info(f\"Loaded {len(self.metadata)} samples\")\n",
        "\n",
        "        # Limit sample size if specified\n",
        "        if sample_size is not None and sample_size < len(self.metadata):\n",
        "            self.metadata = self.metadata[:sample_size]\n",
        "            logger.info(f\"Using {sample_size} samples for testing\")\n",
        "\n",
        "        # Scan first few items to determine the structure of the dataset\n",
        "        self._analyze_metadata_structure()\n",
        "\n",
        "    def _analyze_metadata_structure(self):\n",
        "        \"\"\"Analyze metadata structure to determine video and caption keys\"\"\"\n",
        "        sample_size = min(5, len(self.metadata))\n",
        "        sample_items = self.metadata[:sample_size]\n",
        "\n",
        "        # Check for common video path keys\n",
        "        video_keys = ['video_path', 'video', 'path', 'file_name', 'filename', 'video_file']\n",
        "        caption_keys = ['caption', 'text', 'description', 'captions', 'texts', 'descriptions']\n",
        "\n",
        "        # Count occurrences of each key\n",
        "        video_key_counts = {k: sum(1 for item in sample_items if k in item) for k in video_keys}\n",
        "        caption_key_counts = {k: sum(1 for item in sample_items if k in item) for k in caption_keys}\n",
        "\n",
        "        # Find most common keys\n",
        "        self.video_key = max(video_key_counts.items(), key=lambda x: x[1])[0] if any(video_key_counts.values()) else None\n",
        "        self.caption_key = max(caption_key_counts.items(), key=lambda x: x[1])[0] if any(caption_key_counts.values()) else None\n",
        "\n",
        "        # If not found, try to infer from keys\n",
        "        if self.video_key is None or video_key_counts[self.video_key] == 0:\n",
        "            for item in sample_items:\n",
        "                for k in item.keys():\n",
        "                    if any(vid_term in k.lower() for vid_term in ['video', 'mp4', 'avi', 'mov', 'clip']):\n",
        "                        self.video_key = k\n",
        "                        break\n",
        "            if self.video_key is None:\n",
        "                self.video_key = 'video_path'  # Default\n",
        "\n",
        "        if self.caption_key is None or caption_key_counts[self.caption_key] == 0:\n",
        "            for item in sample_items:\n",
        "                for k in item.keys():\n",
        "                    if any(cap_term in k.lower() for cap_term in ['caption', 'text', 'description']):\n",
        "                        self.caption_key = k\n",
        "                        break\n",
        "            if self.caption_key is None:\n",
        "                self.caption_key = 'caption'  # Default\n",
        "\n",
        "        logger.info(f\"Using '{self.video_key}' for video paths and '{self.caption_key}' for captions\")\n",
        "\n",
        "        # Log some sample entries\n",
        "        logger.info(\"Sample entries:\")\n",
        "        for i, item in enumerate(sample_items):\n",
        "            logger.info(f\"Sample {i+1}: Keys={list(item.keys())}\")\n",
        "\n",
        "    def _get_video_path(self, item):\n",
        "        \"\"\"Extract video path from metadata item with fallbacks\"\"\"\n",
        "        if self.video_key in item:\n",
        "            path = item[self.video_key]\n",
        "        else:\n",
        "            # Try different keys\n",
        "            for key in ['video_path', 'video', 'path', 'file_name', 'filename']:\n",
        "                if key in item:\n",
        "                    path = item[key]\n",
        "                    break\n",
        "            else:\n",
        "                # If no standard key works, find any key that might contain a video path\n",
        "                for k, v in item.items():\n",
        "                    if isinstance(v, str) and any(ext in v for ext in ['.mp4', '.avi', '.mov', '.mkv']):\n",
        "                        path = v\n",
        "                        break\n",
        "                else:\n",
        "                    logger.warning(f\"Could not find video path in item: {item}\")\n",
        "                    path = \"video_not_found.mp4\"\n",
        "\n",
        "        # Handle relative vs absolute paths\n",
        "        if not os.path.isabs(path):\n",
        "            path = os.path.join(self.root_dir, path)\n",
        "\n",
        "        return path\n",
        "\n",
        "    def _get_caption(self, item):\n",
        "        \"\"\"Extract caption from metadata item with fallbacks\"\"\"\n",
        "        if self.caption_key in item:\n",
        "            caption = item[self.caption_key]\n",
        "        else:\n",
        "            # Try different keys\n",
        "            for key in ['caption', 'text', 'description', 'captions', 'texts']:\n",
        "                if key in item:\n",
        "                    caption = item[key]\n",
        "                    break\n",
        "            else:\n",
        "                logger.warning(f\"Could not find caption in item: {item}\")\n",
        "                caption = \"No caption available.\"\n",
        "\n",
        "        # Handle list of captions\n",
        "        if isinstance(caption, list):\n",
        "            caption = caption[0]\n",
        "\n",
        "        return caption\n",
        "\n",
        "    def _load_video_frames(self, video_path):\n",
        "        \"\"\"\n",
        "        Load frames from a video file\n",
        "\n",
        "        Args:\n",
        "            video_path: Path to video file\n",
        "\n",
        "        Returns:\n",
        "            List of frames as PIL Images\n",
        "        \"\"\"\n",
        "        try:\n",
        "            if not os.path.exists(video_path):\n",
        "                logger.warning(f\"Video not found: {video_path}\")\n",
        "                # Return black frames as fallback\n",
        "                return [Image.new('RGB', (224, 224), color='black') for _ in range(self.max_frames)]\n",
        "\n",
        "            # Open video file\n",
        "            cap = cv2.VideoCapture(video_path)\n",
        "            if not cap.isOpened():\n",
        "                logger.warning(f\"Could not open video: {video_path}\")\n",
        "                return [Image.new('RGB', (224, 224), color='black') for _ in range(self.max_frames)]\n",
        "\n",
        "            # Get total frames\n",
        "            total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
        "\n",
        "            # Sample frames based on strategy\n",
        "            if self.frame_sampling == 'uniform':\n",
        "                # Uniformly sample frames\n",
        "                if total_frames <= self.max_frames:\n",
        "                    frame_indices = list(range(total_frames))\n",
        "                else:\n",
        "                    frame_indices = np.linspace(0, total_frames - 1, self.max_frames, dtype=int)\n",
        "            elif self.frame_sampling == 'first':\n",
        "                # Take first N frames\n",
        "                frame_indices = list(range(min(total_frames, self.max_frames)))\n",
        "            elif self.frame_sampling == 'random':\n",
        "                # Randomly sample frames\n",
        "                if total_frames <= self.max_frames:\n",
        "                    frame_indices = list(range(total_frames))\n",
        "                else:\n",
        "                    frame_indices = sorted(np.random.choice(total_frames, self.max_frames, replace=False))\n",
        "            else:\n",
        "                raise ValueError(f\"Unknown frame sampling strategy: {self.frame_sampling}\")\n",
        "\n",
        "            # Extract frames\n",
        "            frames = []\n",
        "            for idx in frame_indices:\n",
        "                cap.set(cv2.CAP_PROP_POS_FRAMES, idx)\n",
        "                ret, frame = cap.read()\n",
        "                if ret:\n",
        "                    # Convert BGR to RGB\n",
        "                    frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
        "                    frames.append(Image.fromarray(frame))\n",
        "                else:\n",
        "                    # If frame read failed, add black frame\n",
        "                    frames.append(Image.new('RGB', (224, 224), color='black'))\n",
        "\n",
        "            # Release video\n",
        "            cap.release()\n",
        "\n",
        "            # If we couldn't extract any frames, return black frames\n",
        "            if len(frames) == 0:\n",
        "                logger.warning(f\"No frames extracted from {video_path}\")\n",
        "                frames = [Image.new('RGB', (224, 224), color='black') for _ in range(self.max_frames)]\n",
        "\n",
        "            # Pad or trim to max_frames\n",
        "            if len(frames) < self.max_frames:\n",
        "                # Pad with duplicates of the last frame\n",
        "                frames.extend([frames[-1] if frames else Image.new('RGB', (224, 224), color='black')\n",
        "                              for _ in range(self.max_frames - len(frames))])\n",
        "            elif len(frames) > self.max_frames:\n",
        "                # Trim excess frames\n",
        "                frames = frames[:self.max_frames]\n",
        "\n",
        "            return frames\n",
        "\n",
        "        except Exception as e:\n",
        "            logger.error(f\"Error loading video {video_path}: {e}\")\n",
        "            # Return black frames as fallback\n",
        "            return [Image.new('RGB', (224, 224), color='black') for _ in range(self.max_frames)]\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.metadata)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        \"\"\"Get video frames and caption for index\"\"\"\n",
        "        item = self.metadata[idx]\n",
        "\n",
        "        # Get video path and caption\n",
        "        video_path = self._get_video_path(item)\n",
        "        caption = self._get_caption(item)\n",
        "\n",
        "        # Load video frames\n",
        "        frames = self._load_video_frames(video_path)\n",
        "\n",
        "        # Process frames and text\n",
        "        try:\n",
        "            inputs = self.processor(\n",
        "                images=frames,\n",
        "                text=caption,\n",
        "                return_tensors=\"pt\",\n",
        "                max_length=self.max_text_length,\n",
        "                padding=\"max_length\",\n",
        "                truncation=True\n",
        "            )\n",
        "\n",
        "            # Remove batch dimension added by the processor\n",
        "            for k in inputs:\n",
        "                if isinstance(inputs[k], torch.Tensor):\n",
        "                    inputs[k] = inputs[k].squeeze(0)\n",
        "\n",
        "            return inputs\n",
        "\n",
        "        except Exception as e:\n",
        "            logger.error(f\"Error processing item {idx}: {e}\")\n",
        "            # Return dummy inputs as fallback\n",
        "            dummy_inputs = {\n",
        "                \"pixel_values\": torch.zeros((self.max_frames, 3, 224, 224)),\n",
        "                \"input_ids\": torch.zeros(self.max_text_length, dtype=torch.long),\n",
        "                \"attention_mask\": torch.zeros(self.max_text_length, dtype=torch.long)\n",
        "            }\n",
        "            return dummy_inputs\n",
        "\n",
        "# %% [markdown]\n",
        "# ## 3. Creating a Synthetic Mini Dataset\n",
        "#\n",
        "# Since we can't access the actual Panda-70M dataset, let's create a tiny synthetic dataset to test our pipeline.\n",
        "\n",
        "# %%\n",
        "# Create a mini synthetic dataset for testing\n",
        "def create_synthetic_dataset(output_dir, num_samples=10, video_length=30, image_size=(224, 224)):\n",
        "    \"\"\"\n",
        "    Create a small synthetic dataset mimicking Panda-70M structure\n",
        "\n",
        "    Args:\n",
        "        output_dir: Output directory\n",
        "        num_samples: Number of samples to generate\n",
        "        video_length: Length of each video in frames\n",
        "        image_size: Size of video frames\n",
        "    \"\"\"\n",
        "    os.makedirs(output_dir, exist_ok=True)\n",
        "    os.makedirs(os.path.join(output_dir, \"videos\"), exist_ok=True)\n",
        "\n",
        "    # Define categories and actions for captions\n",
        "    categories = ['cat', 'dog', 'person', 'car', 'bird']\n",
        "    actions = ['running', 'jumping', 'walking', 'sitting', 'dancing']\n",
        "    locations = ['in a park', 'at home', 'on the street', 'by the beach', 'in a forest']\n",
        "\n",
        "    # Generate samples\n",
        "    metadata = []\n",
        "\n",
        "    for i in range(num_samples):\n",
        "        # Create a synthetic video\n",
        "        video_path = os.path.join(output_dir, \"videos\", f\"video_{i}.mp4\")\n",
        "\n",
        "        # Create a simple moving shape video\n",
        "        height, width = image_size\n",
        "        fourcc = cv2.VideoWriter_fourcc(*'mp4v')\n",
        "        out = cv2.VideoWriter(video_path, fourcc, 5.0, (width, height))\n",
        "\n",
        "        # Choose random category\n",
        "        category_idx = np.random.randint(0, len(categories))\n",
        "        action_idx = np.random.randint(0, len(actions))\n",
        "        location_idx = np.random.randint(0, len(locations))\n",
        "\n",
        "        # Generate frames with moving shape\n",
        "        x, y = np.random.randint(50, width-50), np.random.randint(50, height-50)\n",
        "        dx, dy = np.random.randint(-5, 5), np.random.randint(-5, 5)\n",
        "\n",
        "        color = tuple(np.random.randint(0, 255, 3).tolist())\n",
        "\n",
        "        for frame_idx in range(video_length):\n",
        "            # Create frame\n",
        "            frame = np.zeros((height, width, 3), dtype=np.uint8)\n",
        "\n",
        "            # Update position\n",
        "            x += dx\n",
        "            y += dy\n",
        "\n",
        "            # Bounce off edges\n",
        "            if x < 0 or x > width-30:\n",
        "                dx = -dx\n",
        "                x += 2*dx\n",
        "            if y < 0 or y > height-30:\n",
        "                dy = -dy\n",
        "                y += 2*dy\n",
        "\n",
        "            # Draw shape based on category\n",
        "            if categories[category_idx] == 'cat':\n",
        "                # Draw cat-like shape\n",
        "                cv2.circle(frame, (x, y), 20, color, -1)  # Head\n",
        "                cv2.circle(frame, (x-15, y-15), 10, color, -1)  # Left ear\n",
        "                cv2.circle(frame, (x+15, y-15), 10, color, -1)  # Right ear\n",
        "            elif categories[category_idx] == 'dog':\n",
        "                # Draw dog-like shape\n",
        "                cv2.circle(frame, (x, y), 20, color, -1)  # Head\n",
        "                cv2.circle(frame, (x-20, y-5), 15, color, -1)  # Left ear\n",
        "                cv2.circle(frame, (x+20, y-5), 15, color, -1)  # Right ear\n",
        "            elif categories[category_idx] == 'person':\n",
        "                # Draw person-like shape\n",
        "                cv2.circle(frame, (x, y-15), 15, color, -1)  # Head\n",
        "                cv2.rectangle(frame, (x-10, y), (x+10, y+30), color, -1)  # Body\n",
        "            elif categories[category_idx] == 'car':\n",
        "                # Draw car-like shape\n",
        "                cv2.rectangle(frame, (x-25, y-10), (x+25, y+10), color, -1)  # Body\n",
        "                cv2.circle(frame, (x-15, y+10), 8, (0, 0, 0), -1)  # Left wheel\n",
        "                cv2.circle(frame, (x+15, y+10), 8, (0, 0, 0), -1)  # Right wheel\n",
        "            elif categories[category_idx] == 'bird':\n",
        "                # Draw bird-like shape\n",
        "                cv2.circle(frame, (x, y), 15, color, -1)  # Body\n",
        "                pts = np.array([[x+15, y], [x+30, y-10], [x+30, y+10]], np.int32)\n",
        "                pts = pts.reshape((-1, 1, 2))\n",
        "                cv2.fillPoly(frame, [pts], color)  # Wing\n",
        "\n",
        "            # Write frame\n",
        "            out.write(frame)\n",
        "\n",
        "        # Release video writer\n",
        "        out.release()\n",
        "\n",
        "        # Generate caption\n",
        "        caption = f\"A {categories[category_idx]} {actions[action_idx]} {locations[location_idx]}\"\n",
        "\n",
        "        # Add to metadata\n",
        "        metadata.append({\n",
        "            \"video_path\": f\"videos/video_{i}.mp4\",\n",
        "            \"caption\": caption\n",
        "        })\n",
        "\n",
        "    # Save metadata\n",
        "    with open(os.path.join(output_dir, \"metadata.json\"), \"w\") as f:\n",
        "        json.dump(metadata, f, indent=4)\n",
        "\n",
        "    # Also save as jsonl for compatibility\n",
        "    with open(os.path.join(output_dir, \"metadata.jsonl\"), \"w\") as f:\n",
        "        for item in metadata:\n",
        "            f.write(json.dumps(item) + \"\\n\")\n",
        "\n",
        "    logger.info(f\"Created synthetic dataset with {num_samples} samples at {output_dir}\")\n",
        "    return output_dir\n",
        "\n",
        "# %%\n",
        "# Create a small synthetic dataset in memory\n",
        "SYNTHETIC_DATASET_PATH = \"./synthetic_panda70m\"\n",
        "create_synthetic_dataset(SYNTHETIC_DATASET_PATH, num_samples=5)\n",
        "\n",
        "# %% [markdown]\n",
        "# ## 4. Model Implementation\n",
        "#\n",
        "# Now, let's implement our video-to-text generation model.\n",
        "\n",
        "# %%\n",
        "class VideoToTextModel(nn.Module):\n",
        "    def __init__(self, model_name=\"google/siglip-base-patch16-224\",\n",
        "                 decoder_name=\"gpt2\",\n",
        "                 freeze_encoder=True):\n",
        "        \"\"\"\n",
        "        Video-to-Text generation model combining a vision encoder with a text decoder\n",
        "\n",
        "        Args:\n",
        "            model_name: Name of the vision encoder model\n",
        "            decoder_name: Name of the text decoder model\n",
        "            freeze_encoder: Whether to freeze the encoder weights\n",
        "        \"\"\"\n",
        "        super().__init__()\n",
        "\n",
        "        # Initialize vision encoder\n",
        "        try:\n",
        "            from transformers import AutoModel\n",
        "            self.vision_encoder = AutoModel.from_pretrained(model_name)\n",
        "        except:\n",
        "            logger.info(f\"Could not load {model_name} as AutoModel, trying AutoModelForVision2Seq...\")\n",
        "            self.vision_encoder = AutoModelForVision2Seq.from_pretrained(model_name)\n",
        "\n",
        "        # Initialize text decoder\n",
        "        self.text_decoder = AutoModelForCausalLM.from_pretrained(decoder_name)\n",
        "\n",
        "        # Determine embedding dimensions\n",
        "        vision_dim = self.vision_encoder.config.hidden_size if hasattr(self.vision_encoder.config, 'hidden_size') else 768\n",
        "        text_dim = self.text_decoder.config.hidden_size if hasattr(self.text_decoder.config, 'hidden_size') else 768\n",
        "\n",
        "        # Projection layer to connect the vision encoder to the text decoder\n",
        "        self.projection = nn.Linear(vision_dim, text_dim)\n",
        "\n",
        "        # Freeze encoder if specified\n",
        "        if freeze_encoder:\n",
        "            for param in self.vision_encoder.parameters():\n",
        "                param.requires_grad = False\n",
        "\n",
        "    def forward(self, pixel_values, input_ids=None, attention_mask=None, labels=None):\n",
        "        \"\"\"\n",
        "        Forward pass\n",
        "\n",
        "        Args:\n",
        "            pixel_values: Tensor of shape (batch_size, num_frames, 3, height, width)\n",
        "            input_ids: Token ids for text\n",
        "            attention_mask: Attention mask for text\n",
        "            labels: Labels for text generation\n",
        "\n",
        "        Returns:\n",
        "            dict with loss and logits\n",
        "        \"\"\"\n",
        "        batch_size, num_frames, channels, height, width = pixel_values.shape\n",
        "\n",
        "        # Reshape for vision encoder input (batch_size * num_frames, 3, height, width)\n",
        "        pixel_values = pixel_values.view(-1, channels, height, width)\n",
        "\n",
        "        # Get vision features\n",
        "        vision_outputs = self.vision_encoder(pixel_values=pixel_values).last_hidden_state\n",
        "\n",
        "        # Mean pooling over sequence dimension (if needed)\n",
        "        if len(vision_outputs.shape) > 2:  # If output has sequence dimension\n",
        "            vision_features = vision_outputs[:, 0]  # Use CLS token\n",
        "        else:\n",
        "            vision_features = vision_outputs\n",
        "\n",
        "        # Reshape back to (batch_size, num_frames, hidden_dim)\n",
        "        vision_features = vision_features.view(batch_size, num_frames, -1)\n",
        "\n",
        "        # Mean pool over frames to get video representation\n",
        "        video_features = vision_features.mean(dim=1)  # (batch_size, hidden_dim)\n",
        "\n",
        "        # Project to text decoder dimension\n",
        "        projected_features = self.projection(video_features)  # (batch_size, text_dim)\n",
        "\n",
        "        # Forward pass through text decoder with video features as conditioning\n",
        "        # The exact mechanism depends on the decoder architecture\n",
        "        if input_ids is not None:\n",
        "            # For training or teacher forcing\n",
        "            decoder_outputs = self.text_decoder(\n",
        "                input_ids=input_ids,\n",
        "                attention_mask=attention_mask,\n",
        "                labels=labels if labels is not None else input_ids,\n",
        "                past_key_values=None  # In a full implementation, projected_features would be used here\n",
        "            )\n",
        "\n",
        "            return decoder_outputs\n",
        "        else:\n",
        "            # For inference\n",
        "            return self.text_decoder.generate(\n",
        "                input_ids=torch.ones((batch_size, 1), dtype=torch.long, device=projected_features.device) * self.text_decoder.config.bos_token_id,\n",
        "                max_length=50,\n",
        "                do_sample=True,\n",
        "                top_p=0.9,\n",
        "                top_k=50\n",
        "            )\n",
        "\n",
        "# %% [markdown]\n",
        "# ## 5. Training Function\n",
        "\n",
        "# %%\n",
        "def train_model(model, train_dataloader, val_dataloader=None,\n",
        "                num_epochs=3, learning_rate=5e-5, weight_decay=0.01,\n",
        "                output_dir=\"./output\", device=\"cuda\"):\n",
        "    \"\"\"\n",
        "    Train the video-to-text model\n",
        "\n",
        "    Args:\n",
        "        model: VideoToTextModel instance\n",
        "        train_dataloader: Training data loader\n",
        "        val_dataloader: Validation data loader\n",
        "        num_epochs: Number of training epochs\n",
        "        learning_rate: Learning rate\n",
        "        weight_decay: Weight decay\n",
        "        output_dir: Directory to save model checkpoints\n",
        "        device: Device to train on\n",
        "    \"\"\"\n",
        "    model.to(device)\n",
        "\n",
        "    # Create output directory\n",
        "    os.makedirs(output_dir, exist_ok=True)\n",
        "\n",
        "    # Create optimizer and scheduler\n",
        "    optimizer = AdamW(model.parameters(), lr=learning_rate, weight_decay=weight_decay)\n",
        "    scheduler = CosineAnnealingLR(optimizer, T_max=num_epochs * len(train_dataloader))\n",
        "\n",
        "    # Training loop\n",
        "    best_val_loss = float('inf')\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "        # Training\n",
        "        model.train()\n",
        "        train_loss = 0.0\n",
        "\n",
        "        progress_bar = tqdm(train_dataloader, desc=f\"Epoch {epoch+1}/{num_epochs} [Train]\")\n",
        "        for batch in progress_bar:\n",
        "            # Move batch to device\n",
        "            batch = {k: v.to(device) if isinstance(v, torch.Tensor) else v for k, v in batch.items()}\n",
        "\n",
        "            # Clear gradients\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            # Forward pass\n",
        "            outputs = model(\n",
        "                pixel_values=batch[\"pixel_values\"],\n",
        "                input_ids=batch[\"input_ids\"],\n",
        "                attention_mask=batch[\"attention_mask\"],\n",
        "                labels=batch[\"input_ids\"]  # Use input_ids as labels for text generation\n",
        "            )\n",
        "\n",
        "            loss = outputs.loss\n",
        "\n",
        "            # Backward pass\n",
        "            loss.backward()\n",
        "\n",
        "            # Update parameters\n",
        "            optimizer.step()\n",
        "            scheduler.step()\n",
        "\n",
        "            # Update loss\n",
        "            train_loss += loss.item()\n",
        "\n",
        "            # Update progress bar\n",
        "            progress_bar.set_postfix({\"loss\": loss.item()})\n",
        "\n",
        "        # Calculate average training loss\n",
        "        avg_train_loss = train_loss / len(train_dataloader)\n",
        "        logger.info(f\"Epoch {epoch+1}/{num_epochs} - Avg. Training Loss: {avg_train_loss:.4f}\")\n",
        "\n",
        "        # Validation\n",
        "        if val_dataloader is not None:\n",
        "            model.eval()\n",
        "            val_loss = 0.0\n",
        "\n",
        "            with torch.no_grad():\n",
        "                progress_bar = tqdm(val_dataloader, desc=f\"Epoch {epoch+1}/{num_epochs} [Val]\")\n",
        "                for batch in progress_bar:\n",
        "                    # Move batch to device\n",
        "                    batch = {k: v.to(device) if isinstance(v, torch.Tensor) else v for k, v in batch.items()}\n",
        "\n",
        "                    # Forward pass\n",
        "                    outputs = model(\n",
        "                        pixel_values=batch[\"pixel_values\"],\n",
        "                        input_ids=batch[\"input_ids\"],\n",
        "                        attention_mask=batch[\"attention_mask\"],\n",
        "                        labels=batch[\"input_ids\"]  # Use input_ids as labels for text generation\n",
        "                    )\n",
        "\n",
        "                    loss = outputs.loss\n",
        "\n",
        "                    # Update loss\n",
        "                    val_loss += loss.item()\n",
        "\n",
        "                    # Update progress bar\n",
        "                    progress_bar.set_postfix({\"loss\": loss.item()})\n",
        "\n",
        "            # Calculate average validation loss\n",
        "            avg_val_loss = val_loss / len(val_dataloader)\n",
        "            logger.info(f\"Epoch {epoch+1}/{num_epochs} - Avg. Validation Loss: {avg_val_loss:.4f}\")\n",
        "\n",
        "            # Save best model\n",
        "            if avg_val_loss < best_val_loss:\n",
        "                best_val_loss = avg_val_loss\n",
        "                best_model_path = os.path.join(output_dir, \"best_model.pt\")\n",
        "                torch.save(model.state_dict(), best_model_path)\n",
        "                logger.info(f\"Saved best model to {best_model_path} (loss: {best_val_loss:.4f})\")\n",
        "\n",
        "        # Save checkpoint\n",
        "        checkpoint_path = os.path.join(output_dir, f\"checkpoint_epoch_{epoch+1}.pt\")\n",
        "        torch.save({\n",
        "            \"epoch\": epoch + 1,\n",
        "            \"model_state_dict\": model.state_dict(),\n",
        "            \"optimizer_state_dict\": optimizer.state_dict(),\n",
        "            \"scheduler_state_dict\": scheduler.state_dict(),\n",
        "            \"train_loss\": avg_train_loss,\n",
        "            \"val_loss\": avg_val_loss if val_dataloader else None,\n",
        "        }, checkpoint_path)\n",
        "        logger.info(f\"Saved checkpoint to {checkpoint_path}\")\n",
        "\n",
        "    # Save final model\n",
        "    final_model_path = os.path.join(output_dir, \"final_model.pt\")\n",
        "    torch.save(model.state_dict(), final_model_path)\n",
        "    logger.info(f\"Saved final model to {final_model_path}\")\n",
        "\n",
        "    return model\n",
        "\n",
        "# %% [markdown]\n",
        "# ## 6. Evaluation Function\n",
        "\n",
        "# %%\n",
        "def generate_captions(model, test_dataloader, tokenizer, device=\"cuda\", num_samples=5, max_length=50):\n",
        "    \"\"\"\n",
        "    Generate captions for test videos\n",
        "\n",
        "    Args:\n",
        "        model: Trained VideoToTextModel\n",
        "        test_dataloader: Test data loader\n",
        "        tokenizer: Tokenizer for decoding\n",
        "        device: Device to use\n",
        "        num_samples: Number of samples to generate\n",
        "        max_length: Maximum length of generated text\n",
        "\n",
        "    Returns:\n",
        "        List of dictionaries with original and generated captions\n",
        "    \"\"\"\n",
        "    model.to(device)\n",
        "    model.eval()\n",
        "\n",
        "    results = []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for i, batch in enumerate(tqdm(test_dataloader, desc=\"Generating captions\")):\n",
        "            if i >= num_samples:\n",
        "                break\n",
        "\n",
        "            # Move batch to device\n",
        "            batch = {k: v.to(device) if isinstance(v, torch.Tensor) else v for k, v in batch.items()}\n",
        "\n",
        "            # Original caption\n",
        "            original_caption = tokenizer.decode(batch[\"input_ids\"][0], skip_special_tokens=True)\n",
        "\n",
        "            # Generate caption\n",
        "            output_ids = model(\n",
        "                pixel_values=batch[\"pixel_values\"],\n",
        "                input_ids=None  # No input_ids for generation\n",
        "            )\n",
        "\n",
        "            # Decode generated caption\n",
        "            generated_caption = tokenizer.decode(output_ids[0], skip_special_tokens=True)\n",
        "\n",
        "            # Store results\n",
        "            results.append({\n",
        "                \"original_caption\": original_caption,\n",
        "                \"generated_caption\": generated_caption\n",
        "            })\n",
        "\n",
        "    return results\n",
        "\n",
        "# %% [markdown]\n",
        "# ## 7. End-to-End Pipeline\n",
        "\n",
        "# %%\n",
        "# Initialize tokenizer and processor\n",
        "try:\n",
        "    processor = AutoProcessor.from_pretrained(\"google/siglip-base-patch16-224\")\n",
        "    tokenizer = AutoTokenizer.from_pretrained(\"gpt2\")\n",
        "    tokenizer.pad_token = tokenizer.eos_token\n",
        "    processor.tokenizer = tokenizer\n",
        "except Exception as e:\n",
        "    logger.error(f\"Error initializing processor: {e}\")\n",
        "    # Fallback to separate processors\n",
        "    from transformers import ViTImageProcessor\n",
        "    processor = ViTImageProcessor.from_pretrained(\"google/siglip-base-patch16-224\")\n",
        "    tokenizer = AutoTokenizer.from_pretrained(\"gpt2\")\n",
        "    tokenizer.pad_token = tokenizer.eos_token\n",
        "\n",
        "# %%\n",
        "# Create dataset\n",
        "dataset = Panda70MDataset(\n",
        "    root_dir=SYNTHETIC_DATASET_PATH,\n",
        "    processor=processor,\n",
        "    max_frames=8,\n",
        "    split=\"train\"\n",
        ")\n",
        "\n",
        "# Split into train/val\n",
        "train_size = int(0.8 * len(dataset))\n",
        "val_size = len(dataset) - train_size\n",
        "train_dataset, val_dataset = torch.utils.data.random_split(dataset, [train_size, val_size])\n",
        "\n",
        "# Create dataloaders\n",
        "batch_size = 2  # Small for synthetic dataset\n",
        "train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "val_dataloader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
        "\n",
        "# %%\n",
        "# Initialize model\n",
        "model = VideoToTextModel(\n",
        "    model_name=\"google/siglip-base-patch16-224\",\n",
        "    decoder_name=\"gpt2\",\n",
        "    freeze_encoder=True\n",
        ")\n",
        "\n",
        "# %%\n",
        "# Train model\n",
        "OUTPUT_DIR = \"./output\"\n",
        "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
        "\n",
        "model = train_model(\n",
        "    model=model,\n",
        "    train_dataloader=train_dataloader,\n",
        "    val_dataloader=val_dataloader,\n",
        "    num_epochs=2,  # Small for synthetic dataset\n",
        "    learning_rate=5e-5,\n",
        "    output_dir=OUTPUT_DIR,\n",
        "    device=device\n",
        ")\n",
        "\n",
        "# %%\n",
        "# Generate captions\n",
        "results = generate_captions(\n",
        "    model=model,\n",
        "    test_dataloader=val_dataloader,\n",
        "    tokenizer=tokenizer,\n",
        "    device=device,\n",
        "    num_samples=val_size\n",
        ")\n",
        "\n",
        "# Print results\n",
        "for i, result in enumerate(results):\n",
        "    print(f\"Sample {i+1}:\")\n",
        "    print(f\"  Original: {result['original_caption']}\")\n",
        "    print(f\"  Generated: {result['generated_caption']}\")\n",
        "    print()\n",
        "\n",
        "# %% [markdown]\n",
        "# ## 8. How to Use with the Actual Panda-70M Dataset\n",
        "#\n",
        "# To use this pipeline with the actual Panda-70M dataset:\n",
        "#\n",
        "# 1. Download and extract the Panda-70M dataset\n",
        "# 2. Update the `DATASET_PATH` to point to your Panda-70M directory\n",
        "# 3. Increase training parameters (`num_epochs`, `batch_size`, etc.)\n",
        "# 4. Use a larger model if needed (e.g., ViT-L instead of ViT-B)\n",
        "#\n",
        "# Example:\n",
        "# ```python\n",
        "# DATASET_PATH = \"https://drive.google.com/file/d/1pbh8W3qgst9CD7nlPhsH9wmUSWjQlGdW/view?usp=sharing\"\n",
        "# num_epochs = 20\n",
        "# batch_size = 32  # Adjust based on your GPU\n",
        "# ```\n",
        "#\n",
        "# The pipeline is designed to be robust to different dataset formats, so it should work with the actual Panda-70M dataset without major modifications.\n",
        "\n",
        "# %% [markdown]\n",
        "# ## 9. Conclusion\n",
        "#\n",
        "# This notebook demonstrates a complete pipeline for video-to-text generation using the Panda-70M dataset. The key components are:\n",
        "#\n",
        "# 1. A robust dataset class that can handle different dataset formats\n",
        "# 2. A video-to-text model that combines a vision encoder with a text decoder\n",
        "# 3. Training and evaluation functions\n",
        "#\n",
        "# With this pipeline, you can train models to generate descriptive captions for videos, which can be useful for various applications such as video search, accessibility, and content moderation."
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "4b8b62b18ed2428ca96b1e08f531b1da",
            "f43047f8502b4a70a6e2ecae958b4203",
            "c1d8db25c84744d4a1b8df2b9ee57a8a",
            "98246ae331b04e61b9db992bc588c1a4",
            "9ce6e5af21dc498fbb79127f6d509a7b",
            "c160f1d705a144ce934dd4d706a4f36c",
            "e35d9d84758b49d8b8b21eaeeaaa3a80",
            "9e45f5f3e63b49e6ab6e6b09595852b9",
            "4edb7afef49242fdba055647c1d63efb",
            "d274f5da45c048e8b8e84d9320b1b0df",
            "c05d9d3a9c094c4395f1931d53056024"
          ]
        },
        "id": "io3Q5k8XgyCz",
        "outputId": "8e5f2a37-597e-462a-8d7d-ee09d9c02efd"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: torch in /usr/local/lib/python3.11/dist-packages (2.5.1+cu124)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.11/dist-packages (0.20.1+cu124)\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.11/dist-packages (4.48.3)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (4.67.1)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.11/dist-packages (3.10.0)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (2.2.2)\n",
            "Requirement already satisfied: opencv-python in /usr/local/lib/python3.11/dist-packages (4.11.0.86)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch) (3.17.0)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.11/dist-packages (from torch) (4.12.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch) (3.1.5)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch) (2024.10.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch) (9.1.0.70)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.5.8)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /usr/local/lib/python3.11/dist-packages (from torch) (11.2.1.3)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /usr/local/lib/python3.11/dist-packages (from torch) (10.3.5.147)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /usr/local/lib/python3.11/dist-packages (from torch) (11.6.1.9)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /usr/local/lib/python3.11/dist-packages (from torch) (12.3.1.170)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
            "Requirement already satisfied: triton==3.1.0 in /usr/local/lib/python3.11/dist-packages (from torch) (3.1.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch) (1.3.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from torchvision) (1.26.4)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.11/dist-packages (from torchvision) (11.1.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.24.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.28.1)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from transformers) (6.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (2024.11.6)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from transformers) (2.32.3)\n",
            "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.21.0)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.5.3)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (1.3.1)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (4.56.0)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (1.4.8)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (3.2.1)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas) (2025.1)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas) (2025.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.7->matplotlib) (1.17.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch) (3.0.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2025.1.31)\n",
            "Using device: cuda\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Epoch 1/2 [Train]:   0%|          | 0/2 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "4b8b62b18ed2428ca96b1e08f531b1da"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "You have to specify input_ids",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-9-672e53269fca>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    787\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmakedirs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mOUTPUT_DIR\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexist_ok\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    788\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 789\u001b[0;31m model = train_model(\n\u001b[0m\u001b[1;32m    790\u001b[0m     \u001b[0mmodel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    791\u001b[0m     \u001b[0mtrain_dataloader\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrain_dataloader\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-9-672e53269fca>\u001b[0m in \u001b[0;36mtrain_model\u001b[0;34m(model, train_dataloader, val_dataloader, num_epochs, learning_rate, weight_decay, output_dir, device)\u001b[0m\n\u001b[1;32m    602\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    603\u001b[0m             \u001b[0;31m# Forward pass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 604\u001b[0;31m             outputs = model(\n\u001b[0m\u001b[1;32m    605\u001b[0m                 \u001b[0mpixel_values\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"pixel_values\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    606\u001b[0m                 \u001b[0minput_ids\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"input_ids\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1735\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1736\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1737\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1746\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1748\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-9-672e53269fca>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, pixel_values, input_ids, attention_mask, labels)\u001b[0m\n\u001b[1;32m    517\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    518\u001b[0m         \u001b[0;31m# Get vision features\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 519\u001b[0;31m         \u001b[0mvision_outputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvision_encoder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpixel_values\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpixel_values\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlast_hidden_state\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    520\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    521\u001b[0m         \u001b[0;31m# Mean pooling over sequence dimension (if needed)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1735\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1736\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1737\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1746\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1748\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/models/siglip/modeling_siglip.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_ids, pixel_values, attention_mask, position_ids, return_loss, output_attentions, output_hidden_states, return_dict, interpolate_pos_encoding)\u001b[0m\n\u001b[1;32m   1392\u001b[0m         )\n\u001b[1;32m   1393\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1394\u001b[0;31m         text_outputs = self.text_model(\n\u001b[0m\u001b[1;32m   1395\u001b[0m             \u001b[0minput_ids\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minput_ids\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1396\u001b[0m             \u001b[0mattention_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattention_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1735\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1736\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1737\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1746\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1748\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/models/siglip/modeling_siglip.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_ids, attention_mask, position_ids, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m    955\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    956\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0minput_ids\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 957\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"You have to specify input_ids\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    958\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    959\u001b[0m         \u001b[0minput_shape\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minput_ids\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: You have to specify input_ids"
          ]
        }
      ]
    }
  ]
}
{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyP0/dvgxZInkgA8KMTwm30z",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/paws4code/CLIP4Clip/blob/master/Untitled1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GHtRohtJAJMj",
        "outputId": "b9d0a5ca-edd8-40b1-ffe3-031004eefb59"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: decord in /usr/local/lib/python3.11/dist-packages (0.6.0)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.11/dist-packages (2.5.1+cu124)\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.11/dist-packages (4.48.3)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (2.2.2)\n",
            "Requirement already satisfied: av in /usr/local/lib/python3.11/dist-packages (14.2.0)\n",
            "Collecting lmdb\n",
            "  Downloading lmdb-1.6.2-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (1.1 kB)\n",
            "Requirement already satisfied: pyarrow in /usr/local/lib/python3.11/dist-packages (18.1.0)\n",
            "Requirement already satisfied: numpy>=1.14.0 in /usr/local/lib/python3.11/dist-packages (from decord) (1.26.4)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch) (3.17.0)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.11/dist-packages (from torch) (4.12.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch) (3.1.5)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch) (2024.10.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch) (9.1.0.70)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.5.8)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /usr/local/lib/python3.11/dist-packages (from torch) (11.2.1.3)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /usr/local/lib/python3.11/dist-packages (from torch) (10.3.5.147)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /usr/local/lib/python3.11/dist-packages (from torch) (11.6.1.9)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /usr/local/lib/python3.11/dist-packages (from torch) (12.3.1.170)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
            "Requirement already satisfied: triton==3.1.0 in /usr/local/lib/python3.11/dist-packages (from torch) (3.1.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch) (1.3.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.24.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.28.1)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from transformers) (6.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (2024.11.6)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from transformers) (2.32.3)\n",
            "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.21.0)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.5.3)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.11/dist-packages (from transformers) (4.67.1)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas) (2025.1)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas) (2025.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch) (3.0.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2025.1.31)\n",
            "Downloading lmdb-1.6.2-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (297 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m297.8/297.8 kB\u001b[0m \u001b[31m4.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: lmdb\n",
            "Successfully installed lmdb-1.6.2\n",
            "Requirement already satisfied: accelerate in /usr/local/lib/python3.11/dist-packages (1.3.0)\n",
            "Collecting deepspeed\n",
            "  Downloading deepspeed-0.16.4.tar.gz (1.4 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.4/1.4 MB\u001b[0m \u001b[31m15.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: numpy<3.0.0,>=1.17 in /usr/local/lib/python3.11/dist-packages (from accelerate) (1.26.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from accelerate) (24.2)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.11/dist-packages (from accelerate) (5.9.5)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.11/dist-packages (from accelerate) (6.0.2)\n",
            "Requirement already satisfied: torch>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from accelerate) (2.5.1+cu124)\n",
            "Requirement already satisfied: huggingface-hub>=0.21.0 in /usr/local/lib/python3.11/dist-packages (from accelerate) (0.28.1)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.11/dist-packages (from accelerate) (0.5.3)\n",
            "Requirement already satisfied: einops in /usr/local/lib/python3.11/dist-packages (from deepspeed) (0.8.1)\n",
            "Collecting hjson (from deepspeed)\n",
            "  Downloading hjson-3.1.0-py3-none-any.whl.metadata (2.6 kB)\n",
            "Requirement already satisfied: msgpack in /usr/local/lib/python3.11/dist-packages (from deepspeed) (1.1.0)\n",
            "Collecting ninja (from deepspeed)\n",
            "  Downloading ninja-1.11.1.3-py3-none-manylinux_2_12_x86_64.manylinux2010_x86_64.whl.metadata (5.3 kB)\n",
            "Requirement already satisfied: py-cpuinfo in /usr/local/lib/python3.11/dist-packages (from deepspeed) (9.0.0)\n",
            "Requirement already satisfied: pydantic>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from deepspeed) (2.10.6)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from deepspeed) (4.67.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.21.0->accelerate) (3.17.0)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.21.0->accelerate) (2024.10.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.21.0->accelerate) (2.32.3)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.21.0->accelerate) (4.12.2)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic>=2.0.0->deepspeed) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.27.2 in /usr/local/lib/python3.11/dist-packages (from pydantic>=2.0.0->deepspeed) (2.27.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (3.1.5)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (9.1.0.70)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (12.4.5.8)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (11.2.1.3)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (10.3.5.147)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (11.6.1.9)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (12.3.1.170)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (12.4.127)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (12.4.127)\n",
            "Requirement already satisfied: triton==3.1.0 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (3.1.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch>=2.0.0->accelerate) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch>=2.0.0->accelerate) (3.0.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub>=0.21.0->accelerate) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub>=0.21.0->accelerate) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub>=0.21.0->accelerate) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub>=0.21.0->accelerate) (2025.1.31)\n",
            "Downloading hjson-3.1.0-py3-none-any.whl (54 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m54.0/54.0 kB\u001b[0m \u001b[31m4.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading ninja-1.11.1.3-py3-none-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (422 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m422.9/422.9 kB\u001b[0m \u001b[31m32.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hBuilding wheels for collected packages: deepspeed\n",
            "  Building wheel for deepspeed (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for deepspeed: filename=deepspeed-0.16.4-py3-none-any.whl size=1562526 sha256=e75ab25fd214fffc60ecf7e7405aa63227e99193d6035ac3905602ad73c29a38\n",
            "  Stored in directory: /root/.cache/pip/wheels/b3/6c/e5/ccad75c8ade9cb21e74721affd6d17820b1806249aac34f7f0\n",
            "Successfully built deepspeed\n",
            "Installing collected packages: hjson, ninja, deepspeed\n",
            "Successfully installed deepspeed-0.16.4 hjson-3.1.0 ninja-1.11.1.3\n"
          ]
        }
      ],
      "source": [
        "# Core dependencies\n",
        "!pip install decord torch transformers pandas av lmdb pyarrow\n",
        "# Optional (for distributed training)\n",
        "!pip install accelerate deepspeed"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install gdown\n",
        "!gdown \"https://drive.google.com/uc?id=1k7NzU6wVNZYl6NxOhLXE7Hz7OrpzNLgB\" -O panda70m_subset.zip"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "v2y3uGSxBLhx",
        "outputId": "0b61f98c-a6e2-423c-faf8-29269a10ee4b"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: gdown in /usr/local/lib/python3.11/dist-packages (5.2.0)\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.11/dist-packages (from gdown) (4.13.3)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from gdown) (3.17.0)\n",
            "Requirement already satisfied: requests[socks] in /usr/local/lib/python3.11/dist-packages (from gdown) (2.32.3)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from gdown) (4.67.1)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.11/dist-packages (from beautifulsoup4->gdown) (2.6)\n",
            "Requirement already satisfied: typing-extensions>=4.0.0 in /usr/local/lib/python3.11/dist-packages (from beautifulsoup4->gdown) (4.12.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests[socks]->gdown) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests[socks]->gdown) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests[socks]->gdown) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests[socks]->gdown) (2025.1.31)\n",
            "Requirement already satisfied: PySocks!=1.5.7,>=1.5.6 in /usr/local/lib/python3.11/dist-packages (from requests[socks]->gdown) (1.7.1)\n",
            "Downloading...\n",
            "From (original): https://drive.google.com/uc?id=1k7NzU6wVNZYl6NxOhLXE7Hz7OrpzNLgB\n",
            "From (redirected): https://drive.google.com/uc?id=1k7NzU6wVNZYl6NxOhLXE7Hz7OrpzNLgB&confirm=t&uuid=be9b4fd4-b46e-41d9-9277-ea45249a0230\n",
            "To: /content/panda70m_subset.zip\n",
            "100% 114M/114M [00:02<00:00, 54.2MB/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "ls -l /content/"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5_I0wpx-CD6O",
        "outputId": "46a0b500-6d21-4e6b-de7e-afb3af83c1e6"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "total 111584\n",
            "-rw-r--r-- 1 root root 114255340 Oct 15 18:08 panda70m_subset.zip\n",
            "drwxr-xr-x 1 root root      4096 Mar  6 14:29 \u001b[0m\u001b[01;34msample_data\u001b[0m/\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!unzip panda70m_subset.zip -d /content/panda70m_subset/"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c-e63xBQBldy",
        "outputId": "6529905f-bb60-4fbe-f894-b23af8b322be"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Archive:  panda70m_subset.zip\n",
            "  inflating: /content/panda70m_subset/panda70m_training_2m.csv  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Mock dataset with random \"videos\" (delete this if you have real videos)\n",
        "class MockDataset(Dataset):\n",
        "    def __getitem__(self, idx):\n",
        "        fake_video = torch.randn(3, 16, 224, 224)  # 16 frames, 224x224\n",
        "        caption = \"A person is doing something\"\n",
        "        return {\"video\": fake_video, \"caption\": caption}"
      ],
      "metadata": {
        "id": "md5Rkb3tBqmV"
      },
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 4: Training Loop (Updated)\n",
        "def contrastive_loss(video_embeds, text_embeds):\n",
        "    logits = (video_embeds @ text_embeds.T) * model.logit_scale.exp()\n",
        "    return torch.nn.functional.cross_entropy(logits, torch.arange(len(video_embeds)).to(device))\n",
        "\n",
        "for epoch in range(3):\n",
        "    for batch in dataloader:\n",
        "        videos = batch[\"video\"].to(device)  # Shape: (B, C, T, H, W)\n",
        "        captions = batch[\"caption\"]\n",
        "\n",
        "        # Reshape videos to process frames independently\n",
        "        batch_size, channels, num_frames, height, width = videos.shape\n",
        "        videos = videos.permute(0, 2, 1, 3, 4)  # (B, T, C, H, W)\n",
        "        videos = videos.reshape(-1, channels, height, width)  # (B*T, C, H, W)\n",
        "\n",
        "        # Forward pass for video features\n",
        "        video_features = model.get_image_features(videos)  # (B*T, D)\n",
        "        video_features = video_features.reshape(batch_size, num_frames, -1)  # (B, T, D)\n",
        "        video_features = video_features.mean(dim=1)  # (B, D) - Mean pooling\n",
        "\n",
        "        # Process captions\n",
        "        text_inputs = processor(\n",
        "            text=captions,\n",
        "            return_tensors=\"pt\",\n",
        "            padding=True\n",
        "        ).to(device)\n",
        "        text_features = model.get_text_features(**text_inputs)  # (B, D)\n",
        "\n",
        "        # Compute loss\n",
        "        loss = contrastive_loss(video_features, text_features)\n",
        "\n",
        "        # Backward pass\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        print(f\"Epoch {epoch}, Loss: {loss.item():.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cqLqMV2lFGcP",
        "outputId": "0b0a0814-a7ed-4a18-d781-89f82a2a6426"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 0, Loss: 2.0794\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "ls -l /content/panda70m_subset/"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aZnwOCcZCz_i",
        "outputId": "7706f446-bf80-4f6f-fa60-2553f6bfdc5d"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "total 467820\n",
            "-rw-r--r-- 1 root root 479040992 Oct 15 17:19 panda70m_training_2m.csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a small metadata subset (adjust paths as needed)\n",
        "small_metadata = metadata.head(100)\n",
        "small_metadata.to_csv(\"/content/panda70m_subset/small_metadata.csv\", index=False)"
      ],
      "metadata": {
        "id": "-cy19VPhDQpY"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from decord import VideoReader, cpu\n",
        "import numpy as np\n",
        "\n",
        "def extract_frames(video_path, num_frames=16):\n",
        "    try:\n",
        "        vr = VideoReader(video_path, ctx=cpu(0))  # Use GPU if available (e.g., `cuda(0)`)\n",
        "        total_frames = len(vr)\n",
        "        frame_indices = np.linspace(0, total_frames-1, num=num_frames, dtype=int)\n",
        "        frames = vr.get_batch(frame_indices).asnumpy()  # (T, H, W, C)\n",
        "        return frames\n",
        "    except Exception as e:\n",
        "        print(f\"Skipping {video_path} (error: {e})\")\n",
        "        return None"
      ],
      "metadata": {
        "id": "I_MifJsgDTDy"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torch.utils.data import Dataset\n",
        "\n",
        "class Panda70MDataset(Dataset):\n",
        "    def __init__(self, metadata_path, video_dir, num_frames=16):\n",
        "        self.metadata = pd.read_csv(metadata_path)\n",
        "        self.video_dir = video_dir\n",
        "        self.num_frames = num_frames\n",
        "        self.clip_processor = CLIPProcessor.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.metadata)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        video_name = self.metadata.iloc[idx][\"video_name\"]\n",
        "        caption = self.metadata.iloc[idx][\"caption\"]\n",
        "        video_path = os.path.join(self.video_dir, video_name)\n",
        "\n",
        "        # Extract frames\n",
        "        frames = extract_frames(video_path, self.num_frames)\n",
        "        if frames is None:\n",
        "            return None\n",
        "\n",
        "        # Preprocess frames (resize + normalize for CLIP)\n",
        "        processed = self.clip_processor(\n",
        "            images=frames,  # (T, H, W, C)\n",
        "            return_tensors=\"pt\",\n",
        "            do_resize=True,\n",
        "            size=224\n",
        "        )[\"pixel_values\"]  # (T, 3, 224, 224)\n",
        "\n",
        "        # Stack frames into tensor: (T, C, H, W) -> (C, T, H, W)\n",
        "        video_tensor = torch.stack(processed).permute(1, 0, 2, 3)\n",
        "        return {\"video\": video_tensor, \"caption\": caption}"
      ],
      "metadata": {
        "id": "DDdaifFRDVhE"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import CLIPModel\n",
        "\n",
        "model = CLIPModel.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model.to(device)\n",
        "\n",
        "# Data loader for small subset\n",
        "dataset = Panda70MDataset(\n",
        "    metadata_path=\"/content/panda70m_subset/panda70m_training_2m.csv\",\n",
        "    video_dir=\"/content/panda70m_subset/videos/\",\n",
        "    num_frames=16\n",
        ")\n",
        "dataloader = DataLoader(\n",
        "    dataset,\n",
        "    batch_size=8,\n",
        "    collate_fn=lambda x: {  # Simple collate function\n",
        "        \"videos\": torch.stack([item[\"video\"] for item in x]),\n",
        "        \"captions\": [item[\"caption\"] for item in x]\n",
        "    }\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 355
        },
        "id": "GzF8pt-XDZiL",
        "outputId": "34def905-c149-4ea5-c756-2c90c3f39b8c"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'CLIPProcessor' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-25-0a2af0ee5ff5>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;31m# Data loader for small subset\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m dataset = Panda70MDataset(\n\u001b[0m\u001b[1;32m      9\u001b[0m     \u001b[0mmetadata_path\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"/content/panda70m_subset/panda70m_training_2m.csv\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0mvideo_dir\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"/content/panda70m_subset/videos/\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-23-6ab840c69d19>\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, metadata_path, video_dir, num_frames)\u001b[0m\n\u001b[1;32m      7\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvideo_dir\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvideo_dir\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_frames\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnum_frames\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclip_processor\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCLIPProcessor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_pretrained\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"openai/clip-vit-base-patch32\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__len__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'CLIPProcessor' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "optimizer = torch.optim.AdamW(model.parameters(), lr=1e-5)\n",
        "\n",
        "for epoch in range(3):  # Test with 3 epochs\n",
        "    for batch in dataloader:\n",
        "        videos = batch[\"videos\"].to(device)  # (B, C, T, H, W)\n",
        "        captions = batch[\"captions\"]\n",
        "\n",
        "        # Get text tokens\n",
        "        text_inputs = model.processor(\n",
        "            text=captions,\n",
        "            return_tensors=\"pt\",\n",
        "            padding=True\n",
        "        ).to(device)\n",
        "\n",
        "        # Forward pass\n",
        "        video_features = model.get_image_features(videos.flatten(0, 1))  # (B*T, D)\n",
        "        video_features = video_features.unflatten(0, (videos.shape[0], videos.shape[2])).mean(dim=1)  # (B, D)\n",
        "        text_features = model.get_text_features(**text_inputs)\n",
        "\n",
        "        # Contrastive loss\n",
        "        logits = (video_features @ text_features.T) * model.logit_scale.exp()\n",
        "        loss = torch.nn.functional.cross_entropy(logits, torch.arange(len(videos)).to(device))\n",
        "\n",
        "        # Backward pass\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        optimizer.zero_grad()\n",
        "        print(f\"Loss: {loss.item():.4f}\")"
      ],
      "metadata": {
        "id": "xe6GWWjkDcgP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import torch\n",
        "from decord import VideoReader, cpu\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from transformers import CLIPProcessor, CLIPModel\n",
        "import pandas as pd\n",
        "\n",
        "# Configuration\n",
        "NUM_FRAMES = 16  # Match Clip4Clip's input\n",
        "FRAME_SIZE = (224, 224)  # CLIP resolution\n",
        "BATCH_SIZE = 8\n",
        "\n",
        "# Load CLIP processor for normalization + tokenization\n",
        "clip_processor = CLIPProcessor.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
        "\n",
        "# ------------------------------------------\n",
        "# Step 1: Frame Slicing with Decord (GPU-friendly)\n",
        "# ------------------------------------------\n",
        "def extract_frames(video_path, num_frames=NUM_FRAMES):\n",
        "    \"\"\"Uniformly sample frames from a video using Decord.\"\"\"\n",
        "    try:\n",
        "        vr = VideoReader(video_path, ctx=cpu(0))  # Use GPU if available\n",
        "        total_frames = len(vr)\n",
        "        if total_frames < num_frames:\n",
        "            # Repeat frames if video is too short\n",
        "            frame_indices = list(range(total_frames)) * (num_frames // total_frames + 1)\n",
        "            frame_indices = frame_indices[:num_frames]\n",
        "        else:\n",
        "            frame_indices = [int(i * total_frames / num_frames) for i in range(num_frames)]\n",
        "\n",
        "        frames = vr.get_batch(frame_indices).asnumpy()  # (T, H, W, C)\n",
        "        return frames\n",
        "    except Exception as e:\n",
        "        print(f\"Error processing {video_path}: {e}\")\n",
        "        return None\n",
        "\n",
        "# ------------------------------------------\n",
        "# Step 2: Custom Dataset Class\n",
        "# ------------------------------------------\n",
        "class Panda70MDataset(Dataset):\n",
        "    def __init__(self, metadata_csv, video_dir):\n",
        "        self.metadata = pd.read_csv(metadata_csv)\n",
        "        self.video_dir = video_dir\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.metadata)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        video_name = self.metadata.iloc[idx]['video_name']\n",
        "        caption = self.metadata.iloc[idx]['caption']\n",
        "        video_path = os.path.join(self.video_dir, video_name)\n",
        "\n",
        "        # Extract and preprocess frames\n",
        "        frames = extract_frames(video_path)\n",
        "        if frames is None:\n",
        "            return None  # Handle corrupted videos\n",
        "\n",
        "        # Preprocess frames with CLIP's normalization\n",
        "        processed_frames = clip_processor(\n",
        "            images=frames,  # (T, H, W, C)\n",
        "            return_tensors=\"pt\",\n",
        "            do_resize=True,\n",
        "            size=FRAME_SIZE,\n",
        "            do_normalize=True\n",
        "        )[\"pixel_values\"]  # (T, 3, 224, 224)\n",
        "\n",
        "        # Stack frames into tensor: (T, C, H, W) -> (C, T, H, W)\n",
        "        video_tensor = torch.stack(processed_frames).permute(1, 0, 2, 3)\n",
        "\n",
        "        return {\n",
        "            \"video\": video_tensor,\n",
        "            \"caption\": caption\n",
        "        }\n",
        "\n",
        "# ------------------------------------------\n",
        "# Step 3: Data Loader (with collate_fn)\n",
        "# ------------------------------------------\n",
        "def collate_fn(batch):\n",
        "    batch = [b for b in batch if b is not None]  # Filter corrupted videos\n",
        "    videos = torch.stack([item[\"video\"] for item in batch])\n",
        "    captions = [item[\"caption\"] for item in batch]\n",
        "    return {\"videos\": videos, \"captions\": captions}\n",
        "\n",
        "# Initialize dataset and loader\n",
        "metadata_csv = \"path/to/panda70m_subset_metadata.csv\"\n",
        "video_dir = \"path/to/panda70m_subset_videos/\"\n",
        "dataset = Panda70MDataset(metadata_csv, video_dir)\n",
        "dataloader = DataLoader(\n",
        "    dataset,\n",
        "    batch_size=BATCH_SIZE,\n",
        "    collate_fn=collate_fn,\n",
        "    num_workers=4\n",
        ")\n",
        "\n",
        "# ------------------------------------------\n",
        "# Step 4: Clip4Clip Model Setup (Simplified)\n",
        "# ------------------------------------------\n",
        "class Clip4Clip(torch.nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.clip = CLIPModel.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
        "\n",
        "    def forward(self, videos, texts):\n",
        "        # Video: (B, C, T, H, W)\n",
        "        b, c, t, h, w = videos.shape\n",
        "        video_features = []\n",
        "        for frame_idx in range(t):\n",
        "            frame = videos[:, :, frame_idx, :, :]  # (B, C, H, W)\n",
        "            frame_features = self.clip.get_image_features(pixel_values=frame)\n",
        "            video_features.append(frame_features)\n",
        "\n",
        "        # Mean-pool frame features (Clip4Clip's default)\n",
        "        video_features = torch.stack(video_features).mean(dim=0)  # (B, embed_dim)\n",
        "\n",
        "        # Text features\n",
        "        text_inputs = clip_processor(\n",
        "            text=texts,\n",
        "            padding=True,\n",
        "            return_tensors=\"pt\"\n",
        "        )\n",
        "        text_features = self.clip.get_text_features(**text_inputs)\n",
        "\n",
        "        return video_features, text_features\n",
        "\n",
        "model = Clip4Clip()\n",
        "\n",
        "# ------------------------------------------\n",
        "# Step 5: Example Training/Inference Loop\n",
        "# ------------------------------------------\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "model.to(device)\n",
        "\n",
        "for batch in dataloader:\n",
        "    videos = batch[\"videos\"].to(device)  # (B, C, T, H, W)\n",
        "    captions = batch[\"captions\"]\n",
        "\n",
        "    # Forward pass\n",
        "    video_embeds, text_embeds = model(videos, captions)\n",
        "\n",
        "    # Compute contrastive loss (example)\n",
        "    logits = (video_embeds @ text_embeds.T) * model.clip.logit_scale.exp()\n",
        "    targets = torch.arange(len(videos)).to(device)\n",
        "    loss = torch.nn.functional.cross_entropy(logits, targets)\n",
        "\n",
        "    # Backward pass (example)\n",
        "    loss.backward()\n",
        "    # optimizer.step()\n",
        "    # optimizer.zero_grad()"
      ],
      "metadata": {
        "id": "sYkbtx2MATQy"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}
{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOpG9Td29pndry5uvULvScT",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/paws4code/CLIP4Clip/blob/master/CLIP_for_video_text_retrieval%2C_small_sample_on_50_videos_.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lwoCcdsNLvoo",
        "outputId": "94c183ea-22c4-457b-a0b7-c8ff9da473a8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[33mWARNING: Ignoring invalid distribution -orch (/usr/local/lib/python3.10/dist-packages)\u001b[0m\u001b[33m\n",
            "\u001b[0mRequirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (2.6.0+cu118)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.10/dist-packages (0.21.0+cu118)\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.37.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch) (3.16.1)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.10/dist-packages (from torch) (4.12.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch) (3.1.4)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch) (2024.10.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu11==11.8.89 in /usr/local/lib/python3.10/dist-packages (from torch) (11.8.89)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu11==11.8.89 in /usr/local/lib/python3.10/dist-packages (from torch) (11.8.89)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu11==11.8.87 in /usr/local/lib/python3.10/dist-packages (from torch) (11.8.87)\n",
            "Requirement already satisfied: nvidia-cudnn-cu11==9.1.0.70 in /usr/local/lib/python3.10/dist-packages (from torch) (9.1.0.70)\n",
            "Requirement already satisfied: nvidia-cublas-cu11==11.11.3.6 in /usr/local/lib/python3.10/dist-packages (from torch) (11.11.3.6)\n",
            "Requirement already satisfied: nvidia-cufft-cu11==10.9.0.58 in /usr/local/lib/python3.10/dist-packages (from torch) (10.9.0.58)\n",
            "Requirement already satisfied: nvidia-curand-cu11==10.3.0.86 in /usr/local/lib/python3.10/dist-packages (from torch) (10.3.0.86)\n",
            "Requirement already satisfied: nvidia-cusolver-cu11==11.4.1.48 in /usr/local/lib/python3.10/dist-packages (from torch) (11.4.1.48)\n",
            "Requirement already satisfied: nvidia-cusparse-cu11==11.7.5.86 in /usr/local/lib/python3.10/dist-packages (from torch) (11.7.5.86)\n",
            "Requirement already satisfied: nvidia-nccl-cu11==2.21.5 in /usr/local/lib/python3.10/dist-packages (from torch) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu11==11.8.86 in /usr/local/lib/python3.10/dist-packages (from torch) (11.8.86)\n",
            "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.10/dist-packages (from torch) (3.2.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.10/dist-packages (from torch) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy==1.13.1->torch) (1.3.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from torchvision) (1.26.4)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.10/dist-packages (from torchvision) (11.0.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.19.3 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.27.0)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2024.11.6)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.32.3)\n",
            "Requirement already satisfied: tokenizers<0.19,>=0.14 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.15.2)\n",
            "Requirement already satisfied: safetensors>=0.3.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.4.5)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.67.1)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch) (3.0.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2024.12.14)\n",
            "\u001b[33mWARNING: Ignoring invalid distribution -orch (/usr/local/lib/python3.10/dist-packages)\u001b[0m\u001b[33m\n",
            "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution -orch (/usr/local/lib/python3.10/dist-packages)\u001b[0m\u001b[33m\n",
            "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution -orch (/usr/local/lib/python3.10/dist-packages)\u001b[0m\u001b[33m\n",
            "\u001b[0mRequirement already satisfied: ftfy in /usr/local/lib/python3.10/dist-packages (6.3.1)\n",
            "Requirement already satisfied: regex in /usr/local/lib/python3.10/dist-packages (2024.11.6)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (4.67.1)\n",
            "Requirement already satisfied: decord in /usr/local/lib/python3.10/dist-packages (0.6.0)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (2.2.2)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (1.26.4)\n",
            "Requirement already satisfied: wcwidth in /usr/local/lib/python3.10/dist-packages (from ftfy) (0.2.13)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas) (2024.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas) (2024.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
            "\u001b[33mWARNING: Ignoring invalid distribution -orch (/usr/local/lib/python3.10/dist-packages)\u001b[0m\u001b[33m\n",
            "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution -orch (/usr/local/lib/python3.10/dist-packages)\u001b[0m\u001b[33m\n",
            "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution -orch (/usr/local/lib/python3.10/dist-packages)\u001b[0m\u001b[33m\n",
            "\u001b[0mCollecting git+https://github.com/openai/CLIP.git\n",
            "  Cloning https://github.com/openai/CLIP.git to /tmp/pip-req-build-igqb75s8\n",
            "  Running command git clone --filter=blob:none --quiet https://github.com/openai/CLIP.git /tmp/pip-req-build-igqb75s8\n",
            "  Resolved https://github.com/openai/CLIP.git to commit dcba3cb2e2827b402d2701e7e1c7d9fed8a20ef1\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: ftfy in /usr/local/lib/python3.10/dist-packages (from clip==1.0) (6.3.1)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from clip==1.0) (24.2)\n",
            "Requirement already satisfied: regex in /usr/local/lib/python3.10/dist-packages (from clip==1.0) (2024.11.6)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from clip==1.0) (4.67.1)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (from clip==1.0) (2.6.0+cu118)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.10/dist-packages (from clip==1.0) (0.21.0+cu118)\n",
            "Requirement already satisfied: wcwidth in /usr/local/lib/python3.10/dist-packages (from ftfy->clip==1.0) (0.2.13)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch->clip==1.0) (3.16.1)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.10/dist-packages (from torch->clip==1.0) (4.12.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch->clip==1.0) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch->clip==1.0) (3.1.4)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch->clip==1.0) (2024.10.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu11==11.8.89 in /usr/local/lib/python3.10/dist-packages (from torch->clip==1.0) (11.8.89)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu11==11.8.89 in /usr/local/lib/python3.10/dist-packages (from torch->clip==1.0) (11.8.89)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu11==11.8.87 in /usr/local/lib/python3.10/dist-packages (from torch->clip==1.0) (11.8.87)\n",
            "Requirement already satisfied: nvidia-cudnn-cu11==9.1.0.70 in /usr/local/lib/python3.10/dist-packages (from torch->clip==1.0) (9.1.0.70)\n",
            "Requirement already satisfied: nvidia-cublas-cu11==11.11.3.6 in /usr/local/lib/python3.10/dist-packages (from torch->clip==1.0) (11.11.3.6)\n",
            "Requirement already satisfied: nvidia-cufft-cu11==10.9.0.58 in /usr/local/lib/python3.10/dist-packages (from torch->clip==1.0) (10.9.0.58)\n",
            "Requirement already satisfied: nvidia-curand-cu11==10.3.0.86 in /usr/local/lib/python3.10/dist-packages (from torch->clip==1.0) (10.3.0.86)\n",
            "Requirement already satisfied: nvidia-cusolver-cu11==11.4.1.48 in /usr/local/lib/python3.10/dist-packages (from torch->clip==1.0) (11.4.1.48)\n",
            "Requirement already satisfied: nvidia-cusparse-cu11==11.7.5.86 in /usr/local/lib/python3.10/dist-packages (from torch->clip==1.0) (11.7.5.86)\n",
            "Requirement already satisfied: nvidia-nccl-cu11==2.21.5 in /usr/local/lib/python3.10/dist-packages (from torch->clip==1.0) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu11==11.8.86 in /usr/local/lib/python3.10/dist-packages (from torch->clip==1.0) (11.8.86)\n",
            "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.10/dist-packages (from torch->clip==1.0) (3.2.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.10/dist-packages (from torch->clip==1.0) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy==1.13.1->torch->clip==1.0) (1.3.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from torchvision->clip==1.0) (1.26.4)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.10/dist-packages (from torchvision->clip==1.0) (11.0.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch->clip==1.0) (3.0.2)\n",
            "\u001b[33mWARNING: Ignoring invalid distribution -orch (/usr/local/lib/python3.10/dist-packages)\u001b[0m\u001b[33m\n",
            "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution -orch (/usr/local/lib/python3.10/dist-packages)\u001b[0m\u001b[33m\n",
            "\u001b[0m"
          ]
        }
      ],
      "source": [
        "# 1. Install required dependencies\n",
        "!pip install torch torchvision transformers\n",
        "!pip install ftfy regex tqdm decord pandas numpy\n",
        "!pip install git+https://github.com/openai/CLIP.git"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 2. Load pre-trained CLIP model and processor using transformers\n",
        "from transformers import CLIPProcessor, CLIPModel\n",
        "import torch\n",
        "\n",
        "# Initialize model and processor\n",
        "model = CLIPModel.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
        "processor = CLIPProcessor.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
        "model = model.to(\"cuda\")  # Move model to GPU"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zq1m5mLaLxUR",
        "outputId": "9f3998a3-afd6-4d17-ddca-fbf1c5f6ec11"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/huggingface_hub/file_download.py:795: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/huggingface_hub/file_download.py:795: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 3. Core functions for video-text retrieval\n",
        "import torch\n",
        "import cv2\n",
        "import numpy as np\n",
        "from PIL import Image\n",
        "import os\n",
        "\n",
        "# Extract frames from video file\n",
        "def extract_frames(video_path, num_frames=8):\n",
        "    cap = cv2.VideoCapture(video_path)\n",
        "    frames = []\n",
        "    total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
        "    indices = np.linspace(0, total_frames-1, num_frames, dtype=int)\n",
        "\n",
        "    for i in indices:\n",
        "        cap.set(cv2.CAP_PROP_POS_FRAMES, i)\n",
        "        ret, frame = cap.read()\n",
        "        if ret:\n",
        "            frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
        "            frame = Image.fromarray(frame)\n",
        "            frames.append(frame)\n",
        "    cap.release()\n",
        "    return frames\n",
        "\n",
        "# Compute video embedding by averaging frame embeddings\n",
        "def get_video_embedding(model, processor, video_path, num_frames=8):\n",
        "    frames = extract_frames(video_path, num_frames)\n",
        "    if not frames:\n",
        "        return None\n",
        "\n",
        "    with torch.no_grad():\n",
        "        inputs = processor(images=frames, return_tensors=\"pt\").to(\"cuda\")\n",
        "        outputs = model.get_image_features(**inputs)\n",
        "        video_embedding = outputs.mean(dim=0)\n",
        "        video_embedding = video_embedding / video_embedding.norm()\n",
        "\n",
        "    return video_embedding\n",
        "\n",
        "# Compute text embedding\n",
        "def get_text_embedding(model, processor, text):\n",
        "    with torch.no_grad():\n",
        "        inputs = processor(text=text, return_tensors=\"pt\").to(\"cuda\")\n",
        "        outputs = model.get_text_features(**inputs)\n",
        "        text_embedding = outputs[0]\n",
        "        text_embedding = text_embedding / text_embedding.norm()\n",
        "\n",
        "    return text_embedding\n",
        "\n",
        "# Process multiple videos\n",
        "def process_videos(model, processor, video_dir, video_files):\n",
        "    video_embeddings = []\n",
        "    for video_file in video_files:\n",
        "        video_path = os.path.join(video_dir, video_file)\n",
        "        embedding = get_video_embedding(model, processor, video_path)\n",
        "        if embedding is not None:\n",
        "            video_embeddings.append(embedding)\n",
        "    return torch.stack(video_embeddings)\n",
        "\n",
        "# Process multiple text queries\n",
        "def process_texts(model, processor, texts):\n",
        "    text_embeddings = []\n",
        "    for text in texts:\n",
        "        embedding = get_text_embedding(model, processor, text)\n",
        "        text_embeddings.append(embedding)\n",
        "    return torch.stack(text_embeddings)\n",
        "\n",
        "# Calculate similarity and retrieve top-k matches\n",
        "def retrieve_videos(video_embeddings, text_embeddings, video_files, k=5):\n",
        "    similarity = torch.matmul(text_embeddings, video_embeddings.T)\n",
        "\n",
        "    results = []\n",
        "    for i in range(similarity.shape[0]):\n",
        "        values, indices = torch.topk(similarity[i], k=min(k, len(video_files)))\n",
        "        matches = [(video_files[idx], values[j].item()) for j, idx in enumerate(indices)]\n",
        "        results.append(matches)\n",
        "\n",
        "    return results"
      ],
      "metadata": {
        "id": "JaDOz9uxL0jx"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 4. Example usage with sample queries\n",
        "video_dir = \"/content/MSRVTT/videos/all\"\n",
        "video_files = os.listdir(video_dir)[:20]  # Start with a small subset\n",
        "texts = [\"a person cooking\", \"a dog playing\", \"a car driving on a road\"]\n",
        "\n",
        "# Process videos and texts\n",
        "video_embeddings = process_videos(model, processor, video_dir, video_files)\n",
        "text_embeddings = process_texts(model, processor, texts)\n",
        "\n",
        "# Retrieve top videos for each text\n",
        "results = retrieve_videos(video_embeddings, text_embeddings, video_files)\n",
        "\n",
        "# Print results\n",
        "for i, text in enumerate(texts):\n",
        "    print(f\"Query: {text}\")\n",
        "    for video, score in results[i]:\n",
        "        print(f\"  {video}: {score:.4f}\")\n",
        "    print()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aj0kNBN-L4xV",
        "outputId": "dbbf1d16-4d05-447d-8486-3a64eb026a7b"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Query: a person cooking\n",
            "  video4855.mp4: 0.3235\n",
            "  video6340.mp4: 0.2804\n",
            "  video4666.mp4: 0.2769\n",
            "  video8729.mp4: 0.2711\n",
            "  video3071.mp4: 0.2610\n",
            "\n",
            "Query: a dog playing\n",
            "  video5869.mp4: 0.2484\n",
            "  video131.mp4: 0.2407\n",
            "  video9965.mp4: 0.2280\n",
            "  video1852.mp4: 0.2168\n",
            "  video4855.mp4: 0.2139\n",
            "\n",
            "Query: a car driving on a road\n",
            "  video2635.mp4: 0.3043\n",
            "  video131.mp4: 0.2491\n",
            "  video5869.mp4: 0.2434\n",
            "  video7488.mp4: 0.2370\n",
            "  video3071.mp4: 0.2176\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 5. Evaluation on MSRVTT dataset\n",
        "import json\n",
        "\n",
        "# Load annotation file\n",
        "with open('/content/MSRVTT/annotation/MSR_VTT.json', 'r') as f:\n",
        "    msrvtt_data = json.load(f)\n",
        "\n",
        "# Map video IDs to captions\n",
        "video_to_captions = {}\n",
        "for annotation in msrvtt_data['annotations']:\n",
        "    video_id = annotation['image_id']\n",
        "    caption = annotation['caption']\n",
        "    if video_id not in video_to_captions:\n",
        "        video_to_captions[video_id] = []\n",
        "    video_to_captions[video_id].append(caption)\n",
        "\n",
        "# Match to actual video files\n",
        "video_dir = \"/content/MSRVTT/videos/all\"\n",
        "all_video_files = set(os.listdir(video_dir))\n",
        "\n",
        "# Select a subset for evaluation (50 videos)\n",
        "eval_videos = []\n",
        "eval_captions = []\n",
        "for video_id, captions in list(video_to_captions.items())[:50]:\n",
        "    numeric_id = video_id.replace(\"video\", \"\")\n",
        "    filename = f\"video{numeric_id}.mp4\"\n",
        "\n",
        "    if filename in all_video_files:\n",
        "        eval_videos.append(filename)\n",
        "        eval_captions.append(captions[0])  # Just use first caption\n",
        "\n",
        "# Get embeddings for videos and captions\n",
        "video_embeddings = []\n",
        "for video_file in eval_videos:\n",
        "    video_path = os.path.join(video_dir, video_file)\n",
        "    emb = get_video_embedding(model, processor, video_path)\n",
        "    if emb is not None:\n",
        "        video_embeddings.append(emb)\n",
        "\n",
        "# Match successful videos with captions\n",
        "successful_videos = eval_videos[:len(video_embeddings)]\n",
        "successful_captions = eval_captions[:len(video_embeddings)]\n",
        "\n",
        "# Calculate text embeddings and similarity matrix\n",
        "text_embeddings = process_texts(model, processor, successful_captions)\n",
        "similarity = torch.matmul(text_embeddings, torch.stack(video_embeddings).T)\n",
        "\n",
        "# Calculate retrieval metrics (R@1, R@5, R@10)\n",
        "r1, r5, r10 = 0, 0, 0\n",
        "for i in range(len(successful_videos)):\n",
        "    correct_scores = similarity[i, i].item()\n",
        "    rank = (similarity[i] > correct_scores).sum().item() + 1\n",
        "    if rank <= 1: r1 += 1\n",
        "    if rank <= 5: r5 += 1\n",
        "    if rank <= 10: r10 += 1\n",
        "\n",
        "total = len(successful_videos)\n",
        "print(f\"Results on {total} videos:\")\n",
        "print(f\"R@1: {r1/total:.4f}\")\n",
        "print(f\"R@5: {r5/total:.4f}\")\n",
        "print(f\"R@10: {r10/total:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "foZ70SzuL-xc",
        "outputId": "5b2d35aa-fe3d-4dc5-c98a-f9bf269403e0"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Results on 50 videos:\n",
            "R@1: 0.6200\n",
            "R@5: 0.8600\n",
            "R@10: 0.9400\n"
          ]
        }
      ]
    }
  ]
}